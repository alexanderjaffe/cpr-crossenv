{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, glob, os, math, re, scipy, ecopy, random\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import defaultdict\n",
    "from skbio.stats.ordination import pcoa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO, SearchIO\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "from ete3 import Tree\n",
    "import seaborn as sns\n",
    "sns.set('notebook')\n",
    "%matplotlib inline \n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmdir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold(gene):\n",
    "    if gene != \"None\":\n",
    "        try: return re.search(\"(.+?)_[0-9]+$\", gene).group(1)\n",
    "        except: print(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"TO_FILL_IN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gather curated genome data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read in curated genome metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning environment/study\n",
    "current = sorted(glob.glob(rootdir + \"metadata/filtered_genome_metadata_curated*\"))[-1]\n",
    "mc = pd.read_csv(current, sep=\",\")\n",
    "mc = mc[~mc[\"newname\"].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define color palette\n",
    "env2color = {\n",
    "    \"freshwater\": \"#5B7DAD\",\n",
    "    \"sediment\": \"#312C29\",\n",
    "    \"marine\": \"#10b5a7\",\n",
    "    \"soil\": \"#7a5d1f\",\n",
    "    \"engineered\": \"#B4B5B4\",\n",
    "    \"animal-associated\": \"#A84726\",\n",
    "    \"hypersaline\": \"#d6960b\",\n",
    "    \"plant-associated\": \"#647d37\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore\n",
    "mcs = mc[[\"taxcat\", \"env_broad\"]]\n",
    "mcs[\"count\"]=1\n",
    "mcg = mcs.groupby([\"taxcat\", \"env_broad\"], as_index=False).count()\n",
    "\n",
    "sns.set_style(\"ticks\", {\"axes.edgecolor\": \"0.8\"})\n",
    "kws = dict(linewidth=.5, edgecolor=\"black\")\n",
    "g = sns.relplot(\"env_broad\", \"taxcat\", data=mcg[mcg[\"taxcat\"]!=\"None\"], size=\"count\", hue=\"env_broad\",\n",
    "    palette=env2color, alpha=1, height=3, aspect=2, sizes=(50,500), **kws, legend=\"brief\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# annotate the genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"protein\")\n",
    "cmdir(rootdir + \"protein/prodigal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate protein files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this again if tax corrected\n",
    "calls = []\n",
    "\n",
    "for key, row in mc.iterrows():\n",
    "    \n",
    "    genome = glob.glob(rootdir + \"genomes/\" + row[\"newname\"] + \".fna\")[0]\n",
    "    \n",
    "    # if alternatively coded, repredict proteins with code 25\n",
    "    if row[\"taxcat\"] in [\"Gracilibacteria\", \"Absconditabacteria\"]:\n",
    "        code = \"25\"\n",
    "    else:\n",
    "        code = \"11\"\n",
    "\n",
    "    call = \"prodigal -i \" + genome + \" -m -g \" + code + \" -a \" + \\\n",
    "        rootdir + \"protein/prodigal/\" + os.path.basename(genome).replace(\".fna\", \".fa.genes.faa\") + \\\n",
    "        \" -o \" + rootdir + \"protein/prodigal/\" + os.path.basename(genome).replace(\".fna\", \".fa.genes\") + \\\n",
    "        \" -d \" + rootdir + \"protein/prodigal/\" + os.path.basename(genome).replace(\".fna\", \".fa.genes.fna\")\n",
    "    calls.append(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(call):  \n",
    "    sp.call(call, shell=True)\n",
    "    \n",
    "with ProcessPoolExecutor(20) as executor:\n",
    "    executor.map(run, calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear protein file\n",
    "if os.path.isfile(rootdir + \"/protein/ALL.faa\"):\n",
    "    os.remove(rootdir + \"/protein/ALL.faa\")\n",
    "\n",
    "# concatenate\n",
    "with open(rootdir + \"/protein/ALL.faa\", \"a\") as catfile:\n",
    "    for proteome in glob.glob(rootdir + \"protein/prodigal/*faa\"):\n",
    "        for record in SeqIO.parse(open(proteome), \"fasta\"):\n",
    "            catfile.write(\">\" + str(record.description) + \"\\n\" + str(record.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run kofam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch kofamscan\n",
    "kocall = \"sbatch -J kofamscan --wrap '/path/to/exec_annotation -o \" + rootdir + \"/protein/kofamscan.latest.txt \" + \\\n",
    "    rootdir + \"protein/ALL.faa \" + \"-p path/to/kofam/profiles/prokaryote.hal \" + \\\n",
    "    \"-k /path/to/kofam/metadata/ko_list --cpu 48 -f detail'\"\n",
    "print(kocall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in output\n",
    "buffer = []\n",
    "for line in open(rootdir + \"protein/kofamscan.latest.txt\").readlines():\n",
    "    if \"#\" not in line:\n",
    "        # hilariously long regex\n",
    "        m = re.search(\"[* ]*([\\S]+)\\s+([\\S]+)\\s+([0-9.-]+)\\s+\" + \\\n",
    "            \"([0-9.-]+)\\s+([0-9.+-e]+)\\s(.+?$)\", line.strip())\n",
    "        try:\n",
    "            buffer.append(m.groups())\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "kodf = pd.DataFrame.from_records(buffer, columns =[\"gene\", \"ko\", \"threshold\", \"score\", \"eval\", \"def\"]) \n",
    "buffer=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for significance\n",
    "kodf[\"eval\"] = kodf[\"eval\"].apply(lambda x: float(x))\n",
    "kodf[\"score\"] = kodf[\"score\"].apply(lambda x: float(x))\n",
    "kodf = kodf[kodf[\"eval\"] < 1e-6]\n",
    "# get best hit per gene based on score\n",
    "kfilt = kodf.sort_values('score', ascending=False).drop_duplicates(\"gene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run de-novo protein clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"/scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripts available from\n",
    "# https://github.com/raphael-upmc/proteinClusteringPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"/scripts/runProteinClustering.sh\", \"w\") as wrapper:\n",
    "    # start by subfamily clustering\n",
    "    call1 = \"subfamilies.py --output-directory \" + \\\n",
    "        rootdir + \"/protein/protein_clustering/ --cpu 48 \" + rootdir + \"protein/ALL.faa\"\n",
    "    # then do hmm-hmm comparison to generate families\n",
    "    call2 = \"hhblits.py --cpu 48 \" + \\\n",
    "       rootdir + \"protein/protein_clustering/config.json\"\n",
    "    call3 = \"runningMclClustering.py --force --coverage 0.70 \" + \\\n",
    "        \"--fasta --cpu 48 \" + rootdir + \"protein/protein_clustering/config.json\"\n",
    "    wrapper.write(\"\\n\".join([call1,call2, call3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chmod +x wrapper then sbatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build scaf2bin\n",
    "scaf2bin = {}\n",
    "\n",
    "for genome in glob.glob(rootdir + \"genomes/*\"):\n",
    "    name = os.path.basename(genome).replace(\".fna\", \"\")\n",
    "    for record in SeqIO.parse(open(genome), \"fasta\"):\n",
    "        scaf2bin[record.description.split(\" \")[0]] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaf2bin then filter original df\n",
    "kfilt[\"scaffold\"] = kfilt[\"gene\"].apply(scaffold)\n",
    "kfilt[\"bin\"] = kfilt[\"scaffold\"].map(scaf2bin)\n",
    "kfilt = kfilt[~kfilt[\"bin\"].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce consistency plot from Meheust et al.\n",
    "orf2family = {}\n",
    "count = 0\n",
    "\n",
    "for line in open(rootdir + \"/protein/protein_clustering/orf2family.tsv\").readlines():\n",
    "    if count > 0:\n",
    "        orf2family[line.split(\"\\t\")[0]] = line.split(\"\\t\")[1].strip()\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = []\n",
    "\n",
    "for ko in kfilt[\"ko\"].unique():\n",
    "    \n",
    "    genes = kfilt[kfilt[\"ko\"]==ko][\"gene\"].to_list()\n",
    "    fams = [orf2family[gene] for gene in genes if gene in orf2family]\n",
    "    \n",
    "    if len(genes) > 5:\n",
    "        mode = scipy.stats.mode(fams).mode[0]\n",
    "        p = fams.count(mode)/float(len(genes)) * 100\n",
    "        cons.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(sorted(cons, reverse=True)).reset_index()\n",
    "cdf.columns = [\"rank\", \"consistency\"]\n",
    "sns.set_style(\"ticks\")\n",
    "kws = {'s':20, 'alpha':0.2}\n",
    "sns.regplot(\"rank\", \"consistency\", data=cdf, color=\"blue\", scatter_kws=kws, fit_reg=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconcile with kofams? remember sub threshold hits too\n",
    "\n",
    "fam2ko = {}\n",
    "for fam in glob.glob(rootdir + \"protein/protein_clustering/familiesFasta/*\"):\n",
    "    \n",
    "    orfs = []\n",
    "    for record in SeqIO.parse(open(fam), \"fasta\"):\n",
    "        orfs.append(record.description.split(\" \")[0])\n",
    "    \n",
    "    table = kfilt[kfilt[\"gene\"].isin(orfs)]\n",
    "    \n",
    "    if len(table) > 0:\n",
    "        mode = table[\"ko\"].mode()[0]\n",
    "        p = table[\"ko\"].to_list().count(mode)/float(len(orfs))\n",
    "        #r = np.median(table[table[\"ko\"]==mode][\"score_ratio\"].to_list())\n",
    "        fam2ko[os.path.basename(fam).replace(\".fa\",\"\")] = {\"ko\": mode, \"percent\": round(p,3)} #, \"med_ratio\":round(r,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kps = [fam2ko[key][\"percent\"] for key in fam2ko.keys()]\n",
    "kpf = pd.DataFrame(sorted(kps, reverse=True)).reset_index()\n",
    "kpf.columns = [\"rank\", \"consistency\"]\n",
    "sns.set_style(\"ticks\")\n",
    "kws = {'s':20, 'alpha':0.2}\n",
    "sns.regplot(\"rank\", \"consistency\", data=kpf, color=\"blue\", scatter_kws=kws, fit_reg=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### supp table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_info = defaultdict(list)\n",
    "\n",
    "for family in glob.glob(rootdir + \"protein/protein_clustering/familiesFasta/*\"):\n",
    "    \n",
    "    name = os.path.basename(family).replace(\".fa\",\"\")\n",
    "    sizes = [len(record.seq) for record in SeqIO.parse(open(family), \"fasta\")]\n",
    "    pfam_info[\"family\"].append(name)\n",
    "    pfam_info[\"num_seqs\"].append(len(sizes))\n",
    "    pfam_info[\"median_protein_len\"].append(np.median(sizes))\n",
    "    \n",
    "pfdf = pd.DataFrame(pfam_info)\n",
    "# subset by size\n",
    "pfsub = pfdf[pfdf[\"num_seqs\"]>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfmerge = pfsub.merge(pd.DataFrame.from_dict(fam2ko, orient=\"index\").reset_index().rename(columns={\"index\":\"family\"}), how=\"left\")\n",
    "ko_medians = kfilt.groupby(\"ko\", as_index=False).aggregate({\"score\":\"median\", \"eval\":\"median\"}).rename(columns={\"score\":\"median_score\", \"eval\": \"median_eval\"})\n",
    "pfmerge = pfmerge.merge(ko_medians, how=\"left\").merge(kfilt[[\"ko\", \"threshold\", \"def\"]].drop_duplicates(), how=\"left\").fillna(\"None\").sort_values(\"family\")\n",
    "pfmerge = pfmerge[[\"family\", \"num_seqs\", \"median_protein_len\", \"ko\", \"def\", \"percent\", \"threshold\", \"median_score\", \"median_eval\"]].rename(columns={\"ko\":\"kegg_orthology\", \"def\": \"kegg_definition\", \"percent\": \"fraction_seqs_annotated\", \"threshold\":\"kegg_threshold\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfmerge.to_csv(rootdir + \"protein/supp_table_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making the 16RP tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 16 RP results from kofam results\n",
    "# crappy filter for now - generate terms\n",
    "rp16 = [\"S8\",\"L5\",\"L18\",\"S3\",\"L22\",\"S10\",\"S19\",\"L14\",\"L15\",\"L24\",\"L16\",\"L2\",\"L3\",\"S17\",\"L6\",\"L4\"]\n",
    "terms = [\"subunit ribosomal protein \" + term + \"$\" for term in rp16]\n",
    "# filter\n",
    "k16 = kfilt[kfilt[\"def\"].str.contains('|'.join(terms))]\n",
    "# check results\n",
    "len(k16[\"def\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection step - one per genome, same contig?\n",
    "k16[\"scaffold\"] = k16[\"gene\"].apply(scaffold)\n",
    "k16[\"bin\"] = k16[\"scaffold\"].map(scaf2bin)\n",
    "# get mode scaffold per bin\n",
    "modes = k16.groupby(\"bin\", as_index=False).aggregate({\"scaffold\": lambda x: scipy.stats.mode(x).mode[0]})\n",
    "modes.columns = [\"bin\", \"mode_scaf\"]\n",
    "k16 = k16.merge(modes, how=\"left\", on=\"bin\")\n",
    "# is hmm on the mode scaf?\n",
    "k16[\"scafscore\"] = k16.apply(lambda x: x[\"scaffold\"]==x[\"mode_scaf\"], axis=1)\n",
    "# sort and dereplicate preferencing those on mode scaf\n",
    "rpfilt = k16.sort_values([\"bin\", \"ko\", \"scafscore\", \"score\"], ascending=[False,False,False,False]).drop_duplicates([\"bin\", \"ko\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaf_counts=[]\n",
    "\n",
    "for bin in rpfilt[\"bin\"].unique():\n",
    "    scaf_counts.append(len(set(rpfilt[rpfilt[\"bin\"]==bin][\"scaffold\"])))\n",
    "    \n",
    "sns.distplot(scaf_counts, kde=False, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpiv = rpfilt.pivot(\"bin\", \"ko\", \"gene\").fillna(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build outgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir= rootdir + \"/protein/rp16/\"\n",
    "cmdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in tables\n",
    "supp = pd.read_csv(rootdir + \"protein/bmc_supp.tsv\", sep=\"\\t\")\n",
    "tree = pd.read_csv(rootdir + \"metadata/crossenv_phy.txt\", \n",
    "    header=None, names=[\"phylum\", \"group\"]).fillna(\"None\")\n",
    "\n",
    "# subset\n",
    "references = []\n",
    "# immediate context\n",
    "references += supp[supp[\"revised_tax\"].isin([\"Kazanbacteria\", \"Peregrinibacteria\", \"Peribacteria\", \n",
    "    \"Berkelbacteria\", \"Howlettbacteria\", \"Abawacabacteria\"])][\"name\"].to_list()\n",
    "#superphyla\n",
    "for supergroup in [\"Microgenomates\", \"Parcubacteria\"]:\n",
    "    subtree = tree[tree[\"group\"].str.contains(supergroup)]\n",
    "    subtable = supp[supp[\"revised_tax\"].isin(subtree[\"phylum\"].to_list())]\n",
    "    references += random.sample(subtable[\"name\"].to_list(), 25)\n",
    "    \n",
    "print(len(references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaf2bin\n",
    "scaf2cpr = {}\n",
    "for genome in glob.glob(rootdir + \"genomes/*\"):\n",
    "    for record in SeqIO.parse(open(genome), \"fasta\"):\n",
    "        scaf2cpr[record.description.split(\" \")[0]] = os.path.basename(genome).split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate outgroup files\n",
    "mapping = {\"PF00410\": \"rpS8\",\"PF00281\":\"rpL5\",\"TIGR00060\":\"rpL18\",\"TIGR01009\":\"rpS3\",\n",
    "            \"TIGR01044\":\"rpL22\",\"TIGR01049\":\"rpS10\",\"TIGR01050\":\"rpS19\",\"TIGR01067\":\"rpL14\",\n",
    "            \"TIGR01071\":\"rpL15\",\"TIGR01079\":\"rpL24\",\"TIGR01164\":\"rpL16\",\"TIGR01171\":\"rpL2\",\n",
    "            \"TIGR03625\":\"rpL3\",\"TIGR03635\":\"rpS17\",\"TIGR03654\":\"rpL6\",\"TIGR03953\":\"rpL4\"}\n",
    "\n",
    "for reffile in glob.glob(rootdir + \"reference_genomes/bac175/*.BAC175.concat.faa\"):\n",
    "    if (\"TIGR02013\" not in reffile) and (\"TIGR02386\" not in reffile):\n",
    "        with open(rootdir + \"protein/rp16/\" + mapping[os.path.basename(reffile).split(\".\")[0]] + \".CPR.faa\", \"w\") as out:\n",
    "            for record in SeqIO.parse(open(reffile), \"fasta\"):\n",
    "                if \"@\" not in record.description:\n",
    "                    try: bin = scaf2cpr[scaffold(record.description.split(\" \")[0])]\n",
    "                    except: bin = \"None\"\n",
    "                    if bin in references:\n",
    "                        out.write(\">\" + record.description + \"\\n\" + str(record.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_kegg_mapping = {}\n",
    "reverse_kegg_mapping = {}\n",
    "\n",
    "for key, row in k16.drop_duplicates([\"ko\", \"def\"]).iterrows():\n",
    "    forward_kegg_mapping[row[\"ko\"]] = \"rp\" + row[\"def\"].split(\" \")[-1]\n",
    "    reverse_kegg_mapping[\"rp\" + row[\"def\"].split(\" \")[-1]] = row[\"ko\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate corresponding rpiv\n",
    "outgroups = {}\n",
    "\n",
    "for reffile in glob.glob(rootdir + \"protein/rp16/rp*CPR*\"):\n",
    "    \n",
    "    col = reverse_kegg_mapping[os.path.basename(reffile).split(\".\")[0]]\n",
    "    \n",
    "    for record in SeqIO.parse(open(reffile), \"fasta\"):\n",
    "        \n",
    "        try: taxon = scaf2cpr[scaffold(record.description.split(\" \")[0])]\n",
    "        except: taxon=\"None\"\n",
    "        \n",
    "        if taxon != \"None\":\n",
    "            if taxon not in outgroups:\n",
    "                outgroups[taxon] = {col: record.description.split(\" \")[0]}\n",
    "            else:\n",
    "                outgroups[taxon][col] = record.description.split(\" \")[0]\n",
    "\n",
    "outdf = pd.DataFrame.from_dict(outgroups, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pull + align individual genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir + \"wrapper.sh\", \"w\") as wrapper:\n",
    "    \n",
    "    for col in rpiv.columns:\n",
    "        if \"K0\" in col:# write names\n",
    "            file = outdir + col\n",
    "            with open(file + \".names.txt\", \"w\") as names:\n",
    "                for key, row in rpiv.iterrows():\n",
    "                    if row[col] != \"None\":\n",
    "                        names.write(row[col] + \"\\n\")\n",
    "            # pullseq\n",
    "            call1 = \"pullseq -n \" + file + \".names.txt \" + \\\n",
    "                \"-i \" + rootdir + \"protein/ALL.faa > \" + file + \".faa\"\n",
    "            wrapper.write(call1 + \"\\n\")\n",
    "            \n",
    "            # merge with CPR references for now\n",
    "            reffile = glob.glob(rootdir + \"protein/rp16/\" + forward_kegg_mapping[col] + \".CPR*\")[0]\n",
    "            call2 = \"cat \" + file + \".faa \" + reffile + \\\n",
    "                \" > \" + file + \".concat.faa\"\n",
    "            wrapper.write(call2 + \"\\n\")\n",
    "            \n",
    "            # mafft\n",
    "            call3 = \"mafft --thread 16 --retree 2 --reorder \" + file + \\\n",
    "                \".concat.faa > \" + file + \".mafft\"\n",
    "            wrapper.write(call3 + \"\\n\")\n",
    "            \n",
    "            # bmge\n",
    "            call4 = \"java -jar BMGE.jar -i \" + file + \".mafft\" + \\\n",
    "                \" -t AA -m BLOSUM30 -of \" + file + \".bmge.mafft\"\n",
    "            wrapper.write(call4 + \"\\n\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in outgroups\n",
    "rpiv = pd.concat([rpiv, outdf]).fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_seq = rpiv\n",
    "aln_lens = {}\n",
    "\n",
    "def get_sequence(gene, seq_dict):\n",
    "    if gene==\"None\": \n",
    "        return \"None\"\n",
    "    else:\n",
    "        try: return seq_dict[gene]\n",
    "        except: \n",
    "            print(\"%s not found!\" %(gene))\n",
    "            return \"None\"\n",
    "            \n",
    "# add sequences to merged df\n",
    "for trimmed_alignment in glob.glob(outdir + \"*bmge*\"):\n",
    "    # first read in trimmed sequences\n",
    "    temp_dict = {}\n",
    "    for record in SeqIO.parse(open(trimmed_alignment, \"r\"), \"fasta\"):\n",
    "        # pull clean headers\n",
    "        m = re.search(\"(\\S+).*\", record.description)\n",
    "        temp_dict[m.group(1)] = str(record.seq)\n",
    "    # now add to the dataframe using apply\n",
    "    hmm = os.path.basename(trimmed_alignment).split(\".\")[0]\n",
    "    col_name = hmm + \"_seq\"\n",
    "    merged_seq[col_name] = merged_seq[hmm].apply(lambda x: get_sequence(x, temp_dict))\n",
    "    # get aln len to use later\n",
    "    aln_lens[hmm] = len(record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in counts to use below\n",
    "kos = list(rpfilt[\"ko\"].unique())\n",
    "\n",
    "def count_markers(row):\n",
    "    count = 0\n",
    "    for hmm in kos:\n",
    "        if row[hmm] != \"None\":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "merged_seq[\"rp16_count\"] = merged_seq.apply(count_markers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mins = {\"rp16\": 8}\n",
    "\n",
    "# finally write out the concatenated alignment, pruning previously identified taxa as before\n",
    "for dataset in [\"rp16\"]:\n",
    "    \n",
    "    filename = outdir + dataset + \"_crossenv.mafft\"\n",
    "    with open(filename, \"w\") as outfile:\n",
    "\n",
    "        # for each genome meeting criteria\n",
    "        for key, row in merged_seq.reset_index().iterrows():\n",
    "            # if genome meets min gene count threshes\n",
    "            if (row[dataset + \"_count\"] >= count_mins[dataset]) and (row[\"index\"] not in to_remove):\n",
    "                outfile.write(\">\" + row[\"index\"] + \"\\n\")\n",
    "                # now write out sequences\n",
    "                for hmm in kos:\n",
    "                    col_name = hmm + \"_seq\"\n",
    "                    # if missing gene, just add gaps\n",
    "                    if row[col_name] == \"None\":\n",
    "                        outfile.write(\"-\"*aln_lens[hmm])\n",
    "                    # if gene present\n",
    "                    else:\n",
    "                        outfile.write(row[col_name])\n",
    "                outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and run the trees\n",
    "for concat_align in glob.glob(outdir + \"rp16*mafft*\"):\n",
    "    \n",
    "    basename = os.path.basename(concat_align).split(\".\")[0]\n",
    "    call = \"sbatch -J iqtree --wrap 'iqtree -s \" + concat_align + \" -m MFP -st AA -bb 1000 -nt AUTO -pre \" + concat_align.split(\".\")[0] + \"'\"\n",
    "    #sp.call(call, shell=True)\n",
    "    print(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorate the tree\n",
    "info_dict = {row[\"newname\"]:{\"name\": row[\"name\"], \"env_broad\": row[\"env_broad\"], \"env_narrow\": row[\"env_narrow\"]} for key, row in mc.iterrows()}\n",
    "t = Tree(rootdir + \"protein/rp16/rp16_crossenv.treefile\")\n",
    "itol = open(rootdir + \"/protein/rp16/rp16_crossenv.envbroad.txt\", \"w\")\n",
    "itol.write(\"TREE_COLORS\\nSEPARATOR TAB\\nDATA\\n\")\n",
    "\n",
    "for leaf in t:\n",
    "    oleaf = leaf.name\n",
    "    try:\n",
    "        env_narrow = info_dict[leaf.name][\"env_narrow\"]\n",
    "        color = env2color[info_dict[oleaf][\"env_broad\"]]\n",
    "        label = info_dict[oleaf][\"env_broad\"]\n",
    "    except:\n",
    "        color = \"white\"\n",
    "        label = \"None\"\n",
    "        env_narrow = \"None\"\n",
    "    leaf.name = leaf.name + \"_\" + env_narrow\n",
    "    itol.write(leaf.name + \"\\trange\\t\" + color + \"\\t\" + label + \"\\n\")\n",
    "\n",
    "itol.close()\n",
    "t.write(outfile=rootdir + \"protein/rp16/rp16_crossenv.renamed.treefile\", format=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse phy groups + tree order\n",
    "phy = pd.read_csv(rootdir + \"metadata/crossenv_phy.csv\", header=None)\n",
    "phy.columns = [\"leaf\", \"phy\"]\n",
    "\n",
    "def scrub(leaf):\n",
    "    for cat in list(mc[\"env_narrow\"].unique()) + [\"soda\"]:\n",
    "        leaf = leaf.rsplit(str(cat), 1)[0].strip()\n",
    "    return leaf.replace(\" \", \"_\")\n",
    "\n",
    "phy[\"newname\"] = phy[\"leaf\"].apply(scrub)\n",
    "mc = mc.merge(phy, on=\"newname\", how=\"left\").fillna(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gene content analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overall similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat protein dict\n",
    "pdf = pd.DataFrame.from_dict(orf2family, orient=\"index\").reset_index()\n",
    "pdf.columns = [\"gene\", \"ko\"]\n",
    "pdf[\"bin\"] = pdf[\"gene\"].apply(lambda x: scaf2bin[scaffold(x)])\n",
    "pdf = pdf[~pdf[\"bin\"].isin(to_remove)].drop(\"bin\", axis=1)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pcoa(df, fam_size, phy):\n",
    "    \n",
    "    # reconfigure df\n",
    "    df[\"bin\"] = df[\"gene\"].apply(lambda x: scaf2bin[scaffold(x)])\n",
    "    # add tax names\n",
    "    bin2tax = {row[\"newname\"]: row[\"taxcat\"] for key, row in mc.iterrows()}\n",
    "    df[\"taxcat\"] = df[\"bin\"].map(bin2tax)\n",
    "    \n",
    "    # define dataframe\n",
    "    if phy != \"all\":\n",
    "        sub = df[df[\"taxcat\"]==phy][[\"gene\", \"ko\", \"bin\"]]\n",
    "    else: sub = df[[\"gene\", \"ko\", \"bin\"]]\n",
    "        \n",
    "    gb = sub.groupby([\"bin\", \"ko\"], as_index=False).count()\n",
    "    piv = gb.pivot(\"bin\", \"ko\", \"gene\").fillna(0)\n",
    "    # filter out low count annotations\n",
    "    piv = piv[piv.columns[piv.sum(axis=0) >= fam_size]]\n",
    "    pivb = piv > 0\n",
    "    #calculate distance matrix\n",
    "    jac = ecopy.distance(pivb, method='jaccard', transform='1')\n",
    "    # then use skbio to do pcoA\n",
    "    results = pcoa(jac)\n",
    "    pcresults = pd.DataFrame(results.samples)\n",
    "    pcresults[\"newname\"] = piv.index\n",
    "    \n",
    "    return results, pcresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []\n",
    "\n",
    "for dataset in [\"pclust\"]: #\"kofam\"\n",
    "    \n",
    "    df = kfilt if dataset == \"kofam\" else pdf\n",
    "    full, pcresult = run_pcoa(df, 5, \"Saccharibacteria\")\n",
    "    \n",
    "    for contrast in [\"env_broad\", \"phy\"]:\n",
    "        \n",
    "        rm = pcresult.merge(mc[[\"newname\", contrast]], \n",
    "            on=\"newname\", how=\"left\")[[\"PC1\", \"PC2\", contrast]]\n",
    "        rm.columns = [\"PC1\", \"PC2\", \"contrast\"]\n",
    "        rm[\"dataset\"] = dataset\n",
    "        rm[\"contrast_type\"] = contrast\n",
    "        buffer.append(rm)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure palette\n",
    "phy2color = {phy: sns.color_palette(\"Set2\").as_hex()[i] for i, phy in enumerate(mc[\"phy\"].unique()) if phy!= \"None\"}\n",
    "phy2color[\"None\"] = \"lightgrey\"\n",
    "merged_palette = {**env2color, **phy2color}\n",
    "# add taxcats\n",
    "merged_palette[\"Saccharibacteria\"] = \"#4c72b0\"\n",
    "merged_palette[\"Absconditabacteria\"] = \"#dd8452\"\n",
    "merged_palette[\"Gracilibacteria\"] = \"#55a868\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.set_style(\"ticks\")\n",
    "kws = {'s':50, 'alpha':1, \"edgecolor\":\"black\", \"linewidth\":0.25}\n",
    "#p = {True: \"blue\", False: \"lightgrey\"}\n",
    "g = sns.FacetGrid(pd.concat(buffer).fillna(\"None\"), hue=\"contrast\", palette=merged_palette,\n",
    "    row=\"contrast_type\",height=4, sharex=False, sharey=False, aspect=1.5)\n",
    "g = g.map(sns.scatterplot, \"PC1\", \"PC2\", x_jitter=.01, y_jitter=.01, **kws)\n",
    "g.set_titles('{row_name}').add_legend()\n",
    "plt.savefig(rootdir + \"figures/sac_pcoas.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sac proteome size by environment/quality\n",
    "quality = pd.read_csv(rootdir + \"metadata/genomeInformation.csv\")\n",
    "quality[\"newname\"] = quality[\"genome\"].apply(lambda x: x.replace(\".fna\", \"\"))\n",
    "m = m.merge(quality[[\"newname\", \"completeness\", \"contamination\"]], on=\"newname\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = defaultdict(list)\n",
    "\n",
    "for i in range(70, 100, 5):\n",
    "    \n",
    "    table = m[m[\"completeness\"]>=i]\n",
    "    for key, row in table.iterrows():\n",
    "        comp[\"newname\"].append(row[\"newname\"])\n",
    "        comp[\"taxcat\"].append(row[\"taxcat\"])\n",
    "        comp[\"env_broad\"].append(row[\"env_broad\"])\n",
    "        comp[\"genome_size\"].append(row[\"genome_size\"])\n",
    "        comp[\"orf#\"].append(row[\"orf#\"])\n",
    "        comp[\"threshold\"].append(str(i)+\"%\")\n",
    "\n",
    "compdf = pd.DataFrame(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxcat = \"Saccharibacteria\"\n",
    "order = compdf[(compdf[\"taxcat\"]==taxcat)].query(\"threshold=='95%'\").groupby(\"env_broad\", as_index=False).aggregate({\"orf#\":\"median\"}).sort_values(\"orf#\", ascending=False)[\"env_broad\"].to_list()\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(4,7))\n",
    "sns.boxplot(\"orf#\", \"env_broad\", hue=\"threshold\", order=order, palette=\"Blues\", linewidth=0.5, fliersize=0, \n",
    "    data=compdf[compdf[\"taxcat\"]==taxcat])\n",
    "sns.stripplot(\"orf#\", \"env_broad\", hue=\"threshold\", order=order, color=\"grey\",data=compdf[compdf[\"taxcat\"]==taxcat], dodge=True, size=3)\n",
    "plt.xlabel(\"proteome size (orfs)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.grid('on', which='major', axis='x')\n",
    "plt.savefig(rootdir + \"figures/\" + taxcat.lower() + \"_orfcount.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate color mappings - bin 2 cols for each contrast\n",
    "tax2phy = {\"Saccharibacteria\": \"#4c72b0\", \"Absconditabacteria\": \"#dd8452\", \"Gracilibacteria\": \"#55a868\", \"None\":\"white\"}\n",
    "bin2eb = {i[\"newname\"]: merged_palette[i[\"env_broad\"]] for k,i in mc.iterrows()}\n",
    "bin2group = {i[\"newname\"]: merged_palette[i[\"phy\"]] for k,i in mc.iterrows()}\n",
    "# read in tree order\n",
    "tree_order = phy[\"newname\"].to_list()\n",
    "tdf = pd.DataFrame(tree_order).reset_index()\n",
    "tdf.columns = [\"position\", \"bin\"]\n",
    "bin2tax = {row[\"newname\"]: row[\"taxcat\"] for key, row in mc.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustermap(df, min_fam_size, binary):\n",
    "    \n",
    "    # reconfigure df\n",
    "    sub = df[[\"gene\", \"ko\", \"bin\"]]\n",
    "    gb = sub.groupby([\"bin\", \"ko\"], as_index=False).count()\n",
    "    piv = gb.pivot(\"bin\", \"ko\", \"gene\").fillna(0)\n",
    "    # filter out low count annotations\n",
    "    piv = piv[piv.columns[piv.sum(axis=0) >= min_fam_size]]\n",
    "    if binary == True:\n",
    "        piv = piv > 0\n",
    "        piv = piv.replace(True, 1).reset_index()\n",
    "    else:\n",
    "        piv = piv.replace(0, -99).apply(np.log10).fillna(0)\n",
    "\n",
    "    # reorder\n",
    "    piv = piv.merge(tdf, on=\"bin\", how=\"right\").sort_values(\"position\")\n",
    "    # set row colors\n",
    "    #tax_colors = piv[\"bin\"].map(bin2tax).map(tax2phy)\n",
    "    env_colors = piv[\"bin\"].map(bin2eb)\n",
    "    group_colors = piv[\"bin\"].map(bin2group)\n",
    "    fpiv = piv.drop([\"bin\", \"position\"], axis=1).dropna()\n",
    "    \n",
    "    #plot clustergram\n",
    "    g = sns.clustermap(fpiv, figsize=(15,7), row_cluster=False, method='average', \n",
    "        metric='jaccard', row_colors=[group_colors, env_colors], cmap=\"Blues\",\n",
    "                   cbar_pos = None, dendrogram_ratio=0.07)\n",
    "    for a in g.ax_col_dendrogram.collections:\n",
    "        a.set_linewidth(0.25)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(rootdir + \"figures/heatmap.png\", format=\"png\", dpi=300)\n",
    "    \n",
    "    # return original/clustered column names\n",
    "    return fpiv, g.dendrogram_col.reordered_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, column_indices = clustermap(pdf, 5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find differentially distributed families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as spstats\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = defaultdict(list)\n",
    "\n",
    "for phylum in pdf[\"taxcat\"].unique():\n",
    "    \n",
    "    pvals = []\n",
    "    if phylum !=\"None\":\n",
    "        \n",
    "        table = pdf[pdf[\"taxcat\"]==phylum]\n",
    "        meta = mc[mc[\"taxcat\"]==phylum]\n",
    "        # merge in env data\n",
    "        table = table.merge(mc[[\"newname\", \"env_broad\"]], \n",
    "            left_on=\"bin\", right_on=\"newname\", how=\"left\")\n",
    "\n",
    "        for i, fam in enumerate(table[\"ko\"].unique()):\n",
    "\n",
    "            subtable = table[table[\"ko\"]==fam]\n",
    "\n",
    "            # match with clustergram output\n",
    "            if fam in matrix.columns:\n",
    "\n",
    "                # presence/absence\n",
    "                subtable = subtable.drop_duplicates([\"newname\", \"env_broad\"])\n",
    "\n",
    "                for env in table[\"env_broad\"].unique():\n",
    "                    diffs[\"taxcat\"].append(phylum)\n",
    "                    diffs[\"fam\"].append(fam)\n",
    "                    diffs[\"env\"].append(env)\n",
    "\n",
    "                    in_num = len(subtable[subtable[\"env_broad\"]==env])\n",
    "                    in_total = len(meta[meta[\"env_broad\"]==env])\n",
    "                    out_num = len(subtable[subtable[\"env_broad\"]!=env])\n",
    "                    out_total = len(meta[meta[\"env_broad\"]!=env])\n",
    "                    diffs[\"in_num\"].append(in_num)\n",
    "                    diffs[\"in_perc\"].append(in_num/in_total)\n",
    "                    diffs[\"out_perc\"].append(out_num/out_total)\n",
    "                    \n",
    "                    if out_num ==0: # if exclusive\n",
    "                        diffs[\"ratio\"].append(\"None\")\n",
    "                        diffs[\"exclusive\"].append(True)\n",
    "                    else: # if not exclusive\n",
    "                        diffs[\"ratio\"].append((in_num/in_total)/(out_num/out_total))\n",
    "                        diffs[\"exclusive\"].append(False)\n",
    "                        \n",
    "                    # compute fisher's exact statistic\n",
    "                    contable = [[in_num, out_num], [in_total-in_num, out_total-out_num]]\n",
    "                    oddsratio, pvalue = spstats.fisher_exact(contable, alternative='two-sided')\n",
    "                    diffs[\"fisher_exact\"].append(pvalue)\n",
    "                    pvals.append(pvalue)\n",
    "\n",
    "            print('processed %d of %d for %s\\r'%(i, len(table[\"ko\"].unique()), phylum), end=\"\")\n",
    "            \n",
    "    #fdr correction\n",
    "    diffs[\"fisher_fdr\"] += list(multipletests(pvals, method=\"fdr_bh\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffdf = pd.DataFrame(diffs)\n",
    "diffdf = diffdf.merge(pd.DataFrame.from_dict(fam2ko, orient=\"index\").reset_index().rename(columns={\"index\":\"fam\"}), how=\"left\")\n",
    "diffdf = diffdf.merge(kfilt[[\"ko\", \"def\"]].drop_duplicates(), how=\"left\").fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families_tokeep=[]\n",
    "# exclusive pfams in at least x% of ingroup genomes\n",
    "families_tokeep += diffdf[(diffdf[\"exclusive\"]==True) & (diffdf[\"fisher_fdr\"]<=0.05)][\"fam\"].to_list()\n",
    "# enriched pfams - x% ingroup but at least yfold enrichment\n",
    "both = diffdf[(diffdf[\"ratio\"]!='None') & (diffdf[\"ratio\"]!=0)]\n",
    "families_tokeep += both[(both[\"ratio\"]>=5) & (both[\"fisher_fdr\"]<=0.05)][\"fam\"].to_list()\n",
    "# finally, depleted - majority outgroup + <x% ingroup\n",
    "families_tokeep += diffdf[(diffdf[\"in_perc\"]<=0.10) & (diffdf[\"out_perc\"]>=0.5) & (diffdf[\"fisher_fdr\"]<=0.05)][\"fam\"].to_list()\n",
    "print(len(set(families_tokeep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### supp table 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusive = diffdf[(diffdf[\"exclusive\"]==True) & (diffdf[\"fisher_fdr\"]<=0.05)]\n",
    "exclusive[\"type\"] = \"enriched\"\n",
    "both = diffdf[(diffdf[\"ratio\"]!='None') & (diffdf[\"ratio\"]!=0)]\n",
    "enriched = both[(both[\"ratio\"]>=5) & (both[\"fisher_fdr\"]<=0.05)]\n",
    "enriched[\"type\"] = \"enriched\"\n",
    "depleted = diffdf[(diffdf[\"in_perc\"]<=0.10) & (diffdf[\"out_perc\"]>=0.5) & (diffdf[\"fisher_fdr\"]<=0.05)]\n",
    "depleted[\"type\"] = \"depleted\"\n",
    "s6 = pd.concat([exclusive, enriched, depleted])\n",
    "s6[\"in_perc\"] = s6[\"in_perc\"].apply(lambda x: round(x,4))\n",
    "s6[\"out_perc\"] = s6[\"out_perc\"].apply(lambda x: round(x,4))\n",
    "s6[\"ratio\"] = s6[\"ratio\"].apply(lambda x: round(x,4) if x != \"None\" else \"None\")\n",
    "s6[\"fisher_exact\"] = s6[\"fisher_exact\"].apply(lambda x: round(x,4))\n",
    "s6[\"fisher_fdr\"] = s6[\"fisher_fdr\"].apply(lambda x: round(x,4))\n",
    "s6[\"percent\"] = s6[\"percent\"].apply(lambda x: round(x,4) if x!= \"None\" else \"None\")\n",
    "s6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6 = s6[[\"taxcat\", \"fam\", \"env\", \"type\", \"in_num\", \"in_perc\", \"out_perc\", \"ratio\", \"exclusive\", \"fisher_exact\", \"fisher_fdr\", \"ko\", \"percent\", \"def\"]]\n",
    "s6.columns = [\"lineage\", \"protein_family\", \"habitat_broad\", \"distribution_type\", \"ingroup_num_encoding\", \"ingroup_percent_encoding\", \"outgroup_percent_encoding\",\n",
    "             \"ratio\", \"exclusive\", \"fisher_exact\", \"fisher_fdr\", \"kegg_orthology\", \"fraction_seqs_annotated\", \"kegg_definition\"]\n",
    "s6.sort_values([\"lineage\", \"habitat_broad\", \"distribution_type\", \"ingroup_num_encoding\"], ascending=[True,True, False, False]).to_csv(rootdir + \"protein/supp_table_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process re-ordering info\n",
    "orig = pd.DataFrame(matrix.columns).reset_index()\n",
    "orig.columns = [\"index\", \"fam\"]\n",
    "new = pd.DataFrame(column_indices).reset_index()\n",
    "new.columns = [\"new_index\", \"index\"]\n",
    "inds = orig.merge(new, on=\"index\", how=\"left\")\n",
    "inds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep df\n",
    "table = diffdf.merge(inds[[\"fam\", \"new_index\"]], how=\"left\", on=\"fam\")\n",
    "totaldf = table.groupby(\"fam\", as_index=False).aggregate({\"in_num\":\"sum\"}).rename(columns={\"in_num\":\"fam_total\"})\n",
    "table = table.merge(totaldf, how=\"left\", on=\"fam\")\n",
    "table[\"total_perc\"] = table.apply(lambda x: x[\"in_num\"]/x[\"fam_total\"], axis=1)\n",
    "\n",
    "for contrast in [\"taxcat\", \"env\"]:\n",
    "    \n",
    "    if contrast == \"taxcat\":\n",
    "        color_map = [\"lightgrey\", \"grey\", \"darkgrey\"]\n",
    "    else: color_map = [merged_palette[item] for item in table[\"env\"].unique()]\n",
    "    \n",
    "    grouped = table.groupby([\"fam\",contrast], as_index=False).aggregate({\"total_perc\":\"sum\"})\n",
    "    \n",
    "    if contrast==\"env\":\n",
    "        order = table[\"env\"].unique()\n",
    "    else: order = [\"Absconditabacteria\", \"Gracilibacteria\", \"Saccharibacteria\"]\n",
    "        \n",
    "    grouped.pivot(\"fam\", contrast, \"total_perc\").fillna(0).loc[table.drop_duplicates(\"new_index\").sort_values(\"new_index\")[\"fam\"].to_list(),order].plot.area(color=color_map, figsize=(15,1), legend=False)\n",
    "    sns.despine(left=False, bottom=True)\n",
    "    plt.xlabel(\"\")\n",
    "    #plt.tick_params(axis='x', bottom=False, top=False, labelbottom=False)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(rootdir + \"figures/\" + contrast + \"_ticker.png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"ale\")\n",
    "cmdir(rootdir + \"ale/alignments\")\n",
    "cmdir(rootdir + \"ale/gene_trees\")\n",
    "cmdir(rootdir + \"ale/results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alignment and gene trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtfracs = []\n",
    "\n",
    "with open(rootdir + \"scripts/alignTrimGeneTrees.sh\", \"w\") as wrapper:\n",
    "    \n",
    "    for family in glob.glob(rootdir + \"protein/protein_clustering/familiesFasta/*\"):\n",
    "        \n",
    "        # get fam size\n",
    "        fam_size = len([record for record in SeqIO.parse(open(family), \"fasta\")])\n",
    "        \n",
    "        # subset to diff distributed families\n",
    "        basename = os.path.basename(family).split(\".\")[0]\n",
    "        \n",
    "        if basename in families_tokeep:\n",
    "            \n",
    "            aln_path = rootdir + \"ale/alignments/\" + basename + \".mafft\"\n",
    "            \n",
    "            # get sequence sizes\n",
    "            seq_lens = [len(record) for record in SeqIO.parse(open(family), \"fasta\")]\n",
    "            #for now, filter at > 2 STD below the mean len\n",
    "            thresh = np.mean(seq_lens) - 2*np.std(seq_lens)\n",
    "            # capture how much being lost\n",
    "            filtfracs.append(len([s for s in seq_lens if s<thresh])/float(len(seq_lens)))\n",
    "            \n",
    "            # pullseq call\n",
    "            call = \"pullseq -m \" + str(math.floor(thresh)) + \" -i \" + family + \" > \" + \\\n",
    "                rootdir + \"ale/alignments/\" + basename + \".filtered.faa\"\n",
    "            # mafft call\n",
    "            call1 = \"mafft --thread 16 --retree 2 --reorder \" + \\\n",
    "                rootdir + \"ale/alignments/\" + basename + \".filtered.faa > \" + aln_path\n",
    "            # trimal call\n",
    "            call2 = \"trimal -in \" + aln_path + \" -out \" + aln_path.replace(\".mafft\",\".trimal.mafft\") + \\\n",
    "                \" -gt 0.1\"\n",
    "            wrapper.write(call + \"\\n\" + call1 + \"\\n\" + call2 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many sequences are filtered per fam?\n",
    "sns.distplot(filtfracs, kde=False)\n",
    "plt.xlabel(\"filtered fraction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate calls\n",
    "calls = []\n",
    "\n",
    "for trimal in glob.glob(rootdir + \"ale/alignments/*trimal*\"):\n",
    "    \n",
    "    seqs = [str(record.seq) for record in SeqIO.parse(open(trimal), \"fasta\")]\n",
    "    basename = os.path.basename(trimal).split(\".\")[0]\n",
    "    \n",
    "    if len(set(seqs)) >= 4:\n",
    "        call = \"iqtree -s \" + trimal + \" -bnni -m TEST -st AA -bb 1000 -nt AUTO -pre \" + \\\n",
    "            rootdir + \"/ale/gene_trees/\" + basename\n",
    "    else: # get around iqtree bootstrap limitation\n",
    "        call = \"iqtree -s \" + trimal + \" -bnni -m TEST -st AA -nt AUTO -pre \" + \\\n",
    "            rootdir + \"/ale/gene_trees/\" + basename\n",
    "        \n",
    "    #already done?\n",
    "    if glob.glob(rootdir + \"ale/gene_trees/\" + basename + \".treefile\") == []:\n",
    "        calls.append(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to multiple wrappers\n",
    "n = math.ceil(len(calls)/30)\n",
    "for i in range(0, len(calls),n):\n",
    "    with open(rootdir + \"ale/gene_trees/wrapper\" + \\\n",
    "        str(int(i/n)+1) + \".sh\", \"w\") as wrapper:\n",
    "        for call in calls[i:i + n]:\n",
    "            wrapper.write(call + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then chmod + sbatch in terminal : ` for item in $(ls | grep wrapper); do sbatch -J $item --wrap \"$(pwd)/$item\"; done`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean up species tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outgroups\n",
    "t = Tree(rootdir + \"protein/rp16/rp16_crossenv.treefile\")\n",
    "to_include = [genome for genome in mc[\"newname\"] if genome not in to_remove]\n",
    "t.prune(to_include, preserve_branch_length=True)\n",
    "# reroot using arbitrary abs + gra\n",
    "ancestor = t.get_common_ancestor(\"Shaiber2020_ORALPCFBin00011_Absconditabacteria_36\",\"AR_2015_2-01_BD1-5_23_23_curated\")\n",
    "t.set_outgroup(ancestor)\n",
    "#print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "t.write(outfile=rootdir + \"protein/rp16/rp16_crossenv.cpronly.treefile\", format=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run ale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"ale/scripts\")\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdict = {item:True for item in to_remove}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tree_set in enumerate(glob.glob(rootdir + \"ale/gene_trees/*[0-9].ufboot\")):\n",
    "    \n",
    "    count = 1\n",
    "    cmdir(rootdir + \"ale/temp\")\n",
    "    \n",
    "    # pre-sample 100 trees\n",
    "    all_trees = open(tree_set).readlines()\n",
    "    sample = random.sample(all_trees, 100)\n",
    "    \n",
    "    # remove contaminant genes, modify leaf names\n",
    "    for tree in sample:\n",
    "        \n",
    "        to_keep = []\n",
    "        t = Tree(tree)\n",
    "\n",
    "        for leaf in t:\n",
    "            remove = False\n",
    "            bin = scaf2bin[scaffold(leaf.name)]\n",
    "            try:\n",
    "                trdict[bin]\n",
    "                print(bin)\n",
    "            except KeyError:\n",
    "                to_keep.append(leaf.name)\n",
    "\n",
    "        # write out pruned treefile\n",
    "        t.prune(to_keep,preserve_branch_length=True)\n",
    "        \n",
    "        # then modify leaf names to include species for ALE\n",
    "        for leaf in t:\n",
    "            new = scaf2bin[scaffold(leaf.name)] + \"$\" + leaf.name\n",
    "            leaf.name = new\n",
    "        t.write(outfile=rootdir + \"ale/temp/temp\" + str(count) + \".tre\", format=2)\n",
    "        count+=1\n",
    "    \n",
    "    # concatenate bootstraps\n",
    "    with open(tree_set.replace(\"ufboot\", \"pruned.ufboot\"), \"w\") as out:\n",
    "        for tree in glob.glob(rootdir + \"ale/temp/*\"):\n",
    "            for tree in open(tree).readlines():\n",
    "                out.write(tree + \"\\n\")\n",
    "    \n",
    "    shutil.rmtree(rootdir + \"ale/temp/\")\n",
    "    print('%d of %d trees processed.\\r'%(i, len(glob.glob(rootdir + \"ale/gene_trees/*[0-9].ufboot\"))), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapperize(calls, parts, out):\n",
    "    n = math.ceil(len(calls)/parts)\n",
    "    for i in range(0, len(calls),n):\n",
    "        with open(out + \\\n",
    "            str(int(i/n)+1) + \".txt\", \"w\") as wrapper:\n",
    "            for call in calls[i:i + n]:\n",
    "                wrapper.write(call + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootcalls = []\n",
    "\n",
    "for fam in glob.glob(rootdir + \"ale/gene_trees/*pruned.ufboot\"):\n",
    "    if glob.glob(fam.replace(\"gene_trees\", \"results\") + \"*uml_rec\")==[]:\n",
    "        bootcalls.append(fam)\n",
    "\n",
    "wrapperize(bootcalls, 25, rootdir + \"ale/results/famlist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for callist in glob.glob(rootdir + \"ale/results/famlist*\"):\n",
    "    call = \"sbatch -J \" + os.path.basename(callist).replace(\"fam\",\"\").split(\".\")[0] + \\\n",
    "        \" --wrap 'python \" + rootdir + \"ale/scripts/runAle.py \" + callist + \"'\"\n",
    "    #print(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try running with fraction missing\n",
    "gqual = pd.read_csv(rootdir + \"metadata/genomeInformation.csv\")\n",
    "gqual[\"newname\"] = gqual[\"genome\"].apply(lambda x: x.replace(\".fna\", \"\"))\n",
    "# get species in tree\n",
    "cpr_intree = [leaf.name for leaf in Tree(rootdir + \"protein/rp16/rp16_crossenv.cpronly.treefile\")]\n",
    "gsubset = gqual[gqual[\"newname\"].isin(cpr_intree)]\n",
    "\n",
    "with open(rootdir + \"ale/fraction_missing.txt\", \"w\") as outfile:\n",
    "    for key, row in gsubset.iterrows():\n",
    "        outfile.write(row[\"newname\"] + \":\" + str((1-float(row[\"completeness\"])/100)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ale_results = {}\n",
    "\n",
    "for result in glob.glob(rootdir + \"ale/results/*uml_rec\"):\n",
    "    \n",
    "    buffer = []\n",
    "    name = os.path.basename(result).split(\".\")[0]\n",
    "    \n",
    "    # find tabular portion\n",
    "    for line in open(result).readlines():\n",
    "        elements = line.strip().split(\"\\t\")\n",
    "        if (elements[0] == \"S_terminal_branch\") or (elements[0] == \"S_internal_branch\"):\n",
    "            buffer.append(elements)\n",
    "    ale_results[name] = pd.DataFrame(buffer, columns=[\"branch_type\", \"branch\", \n",
    "        \"duplications\", \"transfers\", \"losses\", \"originations\",\"copies\"], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ale_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over all values\n",
    "cat_results = pd.concat(ale_results.values())\n",
    "sum_events = cat_results.groupby([\"branch\", \"branch_type\"], as_index=False).sum()\n",
    "# exclude terminal branches\n",
    "sum_events = sum_events[sum_events[\"branch_type\"]!=\"S_terminal_branch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### itol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"ale/itol/\")\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cladogram\n",
    "with open(rootdir + \"ale/itol/cladogram.tre\", \"w\") as out:\n",
    "    tree_list = [line for line in open(glob.glob(rootdir + \"ale/results/*uml_rec\")[0]).readlines() if \"S:\\t\" in line]\n",
    "    # reformat internal node names for iTOL\n",
    "    tree_string = tree_list[0].strip().replace(\"S:\\t\", \"\")\n",
    "    tree_mod = re.sub(\"\\)([0-9]+):1\", r\")INT\\1:1\", tree_string)\n",
    "    out.write(tree_mod.replace(\"'\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "def rgba2hex(col):\n",
    "    \n",
    "    def rft(raw):\n",
    "        return int(round(raw*255))\n",
    "    \n",
    "    return '#{:02x}{:02x}{:02x}'.format(rft(col[0]), rft(col[1]), rft(col[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make datasets\n",
    "for var in sum_events.columns:\n",
    "    \n",
    "    if \"branch\" not in var:\n",
    "        \n",
    "        #define color scale\n",
    "        cmap = sns.light_palette(\"orange\", as_cmap=True)\n",
    "        \n",
    "        # write out itol file\n",
    "        itol = open(rootdir + \"ale/itol/\" + var + \".itol.txt\", \"w\")\n",
    "        # change SYMBOL TO STYLE for branch colors\n",
    "        itol.write(\"DATASET_SYMBOL\\nSEPARATOR COMMA\\nDATASET_LABEL,\" + \\\n",
    "            var + \"\\nCOLOR,#d3d3d3\\nDATA\\n\")\n",
    "        for key, row in sum_events.iterrows():\n",
    "            scalar = row[var]/max(sum_events[var])\n",
    "            color = rgba2hex(cmap(scalar))\n",
    "            node = \"INT\" + row[\"branch\"] if \"internal\" in row[\"branch_type\"] else row[\"branch\"]\n",
    "            if row[var] >= 1:\n",
    "                itol.write(\"%s,2,%d,%s,1,0.5\\n\" %(node,scalar*10,color))\n",
    "                #itol.write(\"%s,branch,node,%s,1,normal\\n\" %(node,color))\n",
    "        itol.close()\n",
    "        \n",
    "        # generate standalone color bars\n",
    "        if var not in [\"duplications\", \"copies\"]:\n",
    "            print(var)\n",
    "            title = \"total \" + var if var != \"transfers\" else \"total within-CPR transfers\"\n",
    "            sns.set_style({\"axes.linewidth\":0.25, \"axes.edgecolor\":\"black\"})\n",
    "            fig, ax = plt.subplots(figsize=(0.5,3))\n",
    "            #fig.subplots_adjust(bottom=0.5)\n",
    "            cmapp = cmap\n",
    "            norm = mpl.colors.Normalize(vmin=min(sum_events[sum_events[var]>=1][var]), \n",
    "                vmax=max(sum_events[var]))\n",
    "            fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "                cax=ax, orientation='vertical')\n",
    "            plt.savefig(rootdir + \"ale/itol/\" + var + \".bar.svg\", format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorate the tree\n",
    "itol = open(rootdir + \"ale/itol/cladogram_envbroad.itol.txt\", \"w\")\n",
    "itol.write(\"TREE_COLORS\\nSEPARATOR TAB\\nDATA\\n\")\n",
    "\n",
    "for key, row in mc.iterrows():\n",
    "    color = merged_palette[row[\"env_broad\"]]\n",
    "    label = row[\"env_broad\"]\n",
    "    itol.write(row[\"newname\"] + \"\\trange\\t\" + color + \"\\t\" + label + \"\\n\")\n",
    "itol.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rhodopsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"protein/rhod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.nature.com/articles/s41586-018-0225-9#Sec1 supp\n",
    "with open(rootdir + \"protein/rhod/push_refs_clean.faa\", \"w\") as outfile:\n",
    "    for record in SeqIO.parse(open(rootdir + \"protein/rhod/Supp_Data2_AlignmentFileType1plusHelioRs.txt\"), \"fasta\"):\n",
    "        outfile.write(\">\" + record.description + \"\\n\" + str(record.seq).replace(\"-\", \"\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"protein/rhod/rhod_aln.itol.txt\", \"w\") as outfile:\n",
    "    outfile.write(\"DATASET_ALIGNMENT\\nSEPARATOR COMMA\\nDATASET_LABEL,rhod\\nCOLOR,#ff0000\\nCUSTOM_COLOR_SCHEME,MY_SCHEME_1,A=#d2d0c9,M=#d2d0c9,I=#d2d0c9,L=#d2d0c9,V=#d2d0c9,P=#746f69,G=#746f69,C=#746f69,F=#d0ad16,Y=#d0ad16,W=#d0ad16,S=#34acfb,T=#34acfb,N=#34acfb,-=#ffffff,Q=#34acfb,R=#34fb54,K=#34fb54,H=#34fb54,D=#fb4034,E=#fb4034\\nDATA\\n\")\n",
    "    for record in SeqIO.parse(open(rootdir + \"/protein/rhod/all_push_rhodopsin.stripped.mafft\"), \"fasta\"):\n",
    "        outfile.write(\">\" + record.description.split(\" \")[0] + \"\\n\" + str(record.seq) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
