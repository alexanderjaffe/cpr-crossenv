{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib, re, os, glob, datetime, difflib, random, time, math, json, wget\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import subprocess as sp\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "sns.set('notebook')\n",
    "%matplotlib inline \n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Bio import SeqIO, SeqUtils, SearchIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmdir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "def scaffold(gene):\n",
    "    if gene != \"None\":\n",
    "        try: return re.search(\"(.+?)_[0-9]+$\", gene).group(1)\n",
    "        except: print(gene) \n",
    "            \n",
    "def datestamp():\n",
    "    return date.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define color palette\n",
    "env2color = {\n",
    "    \"freshwater\": \"#5B7DAD\",\n",
    "    \"sediment\": \"#312C29\",\n",
    "    \"marine\": \"#10b5a7\",\n",
    "    \"soil\": \"#7a5d1f\",\n",
    "    \"engineered\": \"#B4B5B4\",\n",
    "    \"animal-associated\": \"#A84726\",\n",
    "    \"hypersaline\": \"#d6960b\",\n",
    "    \"plant-associated\": \"#647d37\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(env2color.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"TO_FILL_IN\"\n",
    "cmdir(rootdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning environment/study\n",
    "current = sorted(glob.glob(rootdir + \"metadata/filtered_genome_metadata_curated*\"))[-1]\n",
    "mc = pd.read_csv(current, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_public = pd.read_csv(rootdir + \"metadata/crossenv_reads_public_2020_10_15_curated.tsv\", sep=\"\\t\").fillna(\"None\")\n",
    "curated_public.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_ggkbase = pd.read_csv(rootdir + \"/metadata/crossenv_reads_ggkbase_2020_10_15_curated.tsv\", sep=\"\\t\").fillna(\"None\")\n",
    "curated_ggkbase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curatedm = pd.concat([curated_public, curated_ggkbase]).fillna(\"None\")\n",
    "curatedm = curatedm.drop(\"env_broad\", axis=1)\n",
    "curatedm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get run information\n",
    "# NB VERY LONG RUNTIME\n",
    "rundata = defaultdict(list)\n",
    "leftovers = {}\n",
    "count=0\n",
    "\n",
    "for key, row in curatedm.iterrows():\n",
    "    \n",
    "    # those with SRA data\n",
    "    if row[\"accession_curated\"]!= \"None\":\n",
    "        \n",
    "        tokens = row[\"accession_curated\"].replace(\" \", \"\").split(\",\")\n",
    "        sra_ids = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            prefix = re.search(\"([A-Z]+)[0-9]+\", token).group(1)\n",
    "            \n",
    "            # first collect SRA ids\n",
    "            if prefix in [\"ERS\", \"SRS\", \"PRJEB\", \"SAMN\", \"PRJNA\"]:\n",
    "                \n",
    "                # retreive sra ID\n",
    "                try:\n",
    "                    handle = Entrez.esearch(db='sra', term=token, RetMax=100)\n",
    "                    result = Entrez.read(handle)\n",
    "                    sra_ids += result['IdList']\n",
    "                    handle.close()\n",
    "                except:\n",
    "                    print(\"Could not retrieve %s\" %(token))\n",
    "            \n",
    "            else: # direct to SRA search\n",
    "                sra_ids.append(token)\n",
    "     \n",
    "        for sra_id in sra_ids:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                if sra_id not in leftovers.keys():\n",
    "                    \n",
    "                    handle = Entrez.efetch(db=\"sra\", id=sra_id, rettype=\"runinfo\", retmode=\"xml\")\n",
    "                    soup = BeautifulSoup(handle, \"lxml\")\n",
    "                    leftovers[sra_id] = soup\n",
    "                \n",
    "                else: \n",
    "                    soup = leftovers[sra_id]\n",
    "\n",
    "                for run in soup.findAll(\"row\"):\n",
    "\n",
    "                    rundata[\"newname\"].append(row[\"newname\"])\n",
    "                    rundata[\"host\"].append(row[\"host\"])\n",
    "                    rundata[\"run_id\"].append(run.find(\"run\").text)\n",
    "                    rundata[\"date\"].append(run.find(\"releasedate\").text)\n",
    "                    rundata[\"spots\"].append(run.find(\"spots\").text)\n",
    "                    rundata[\"bases\"].append(run.find(\"bases\").text)\n",
    "                    rundata[\"library_type\"].append(run.find(\"librarysource\").text)\n",
    "                    rundata[\"library_method\"].append(run.find(\"librarystrategy\").text)\n",
    "                    rundata[\"library_layout\"].append(run.find(\"librarylayout\").text)\n",
    "                    rundata[\"instrument\"].append(run.find(\"model\").text)\n",
    "                    rundata[\"read_path\"].append(row[\"read_path\"])\n",
    "                    rundata[\"assembly_path\"].append(row[\"assembly_path\"])\n",
    "\n",
    "                handle.close()\n",
    "            \n",
    "            except:\n",
    "                print(\"Could not retrieve %s\" %(sra_id))\n",
    "\n",
    "    # those without SRA data but valid ggk reads\n",
    "    elif \"/groups/\" in row[\"read_path\"]:\n",
    "        \n",
    "        read_sets = row[\"read_path\"].replace(\" \", \"\").split(\",\")\n",
    "        \n",
    "        for read_set in read_sets:\n",
    "            \n",
    "            rundata[\"newname\"].append(row[\"newname\"])\n",
    "            rundata[\"host\"].append(row[\"host\"])\n",
    "            rundata[\"run_id\"].append(row[\"bioproject\"])  \n",
    "            rundata[\"date\"].append(\"None\")\n",
    "            rundata[\"spots\"].append(\"None\")\n",
    "            rundata[\"bases\"].append(\"None\")\n",
    "            rundata[\"library_type\"].append(\"METAGENOMIC\")\n",
    "            rundata[\"library_method\"].append(\"WGS\")\n",
    "            rundata[\"library_layout\"].append(\"PAIRED\")\n",
    "            rundata[\"instrument\"].append(\"None\")\n",
    "            rundata[\"read_path\"].append(read_set)\n",
    "            rundata[\"assembly_path\"].append(row[\"assembly_path\"])   \n",
    "    \n",
    "    count+=1\n",
    "    print('%d of %d genomes retrieved.\\r'%(count, len(curatedm)), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rundf = pd.DataFrame(rundata)\n",
    "# filter it down\n",
    "rdf = rundf[(~rundf[\"library_type\"].isin([\"METATRANSCRIPTOMIC\", \"GENOMIC SINGLE CELL\"])) \n",
    "    & (~rundf[\"library_method\"].isin([\"AMPLICON\", \"RNA-Seq\", \"WGA\"]))\n",
    "    #& (~rundf[\"newname\"].isin(to_remove))\n",
    "    & (rundf[\"instrument\"]!=\"MinION\") & (rundf[\"library_layout\"]!=\"SINGLE\")]\n",
    "# recast\n",
    "rdf[\"spots\"] = rdf[\"spots\"].apply(lambda x: int(x) if x!=\"None\" else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out fill rdf \n",
    "rdf.to_csv(rootdir + \"/metadata/rdf.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read processing + mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"scripts\")\n",
    "cmdir(rootdir + \"mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ref genome paths\n",
    "ref_genomes={line.split(\"\\t\")[0]: rootdir + \"reference_genomes/animal/\" + line.split(\"\\t\")[1].strip() \n",
    "    for line in open(rootdir + \"reference_genomes/animal/reference_mappings.tsv\").readlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot read # distribution\n",
    "sns.distplot(rdf.query(\"spots != 'None'\")[\"spots\"], bins=150)\n",
    "# arbitrary cutoff\n",
    "plt.axvline(1E8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perc(value):\n",
    "    \n",
    "    goal = round(np.mean(rdf.query(\"spots != 'None'\")[\"spots\"]))\n",
    "    return round(goal/float(value),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up public read mapping + processing\n",
    "callsets = []\n",
    "\n",
    "for runid in rdf[\"run_id\"].unique():\n",
    "    \n",
    "    temp = []\n",
    "    basename = rootdir + \"mapping/\" + runid\n",
    "    subtable = rdf[rdf[\"run_id\"]==runid]\n",
    "    readname = basename\n",
    "    # define read paths\n",
    "    forward = basename + \".1.fastq.gz\"\n",
    "    reverse = basename + \".2.fastq.gz\"\n",
    "    \n",
    "    if (\"SRR\" in runid) or (\"ERR\" in runid):\n",
    "        \n",
    "        # download run\n",
    "        temp.append(\"echo Starting download of %s... >> %s\" %(runid, basename + \".log\"))\n",
    "        temp.append(\"parallel-fastq-dump -s \" + runid + \" -t 48 -O \" +\\\n",
    "            rootdir + \"mapping --split-files --gzip 2>> \" + basename + \".log\")\n",
    "        # filter if necessary\n",
    "        if subtable[\"spots\"].to_list()[0] !=\"None\":\n",
    "            if subtable[\"spots\"].to_list()[0] > 1e8:\n",
    "                perc = compute_perc(subtable[\"spots\"].to_list()[0])\n",
    "                for side in [\"1\", \"2\"]:\n",
    "                    temp.append(\"seqtk sample -s 7 \" + basename + \"_\" + \\\n",
    "                        side + \".fastq.gz \" + str(perc) + \" > \" + basename + \"_\" + side + \".SUB.fastq\")\n",
    "                    temp.append(\"pigz -p 48 \" + basename + \"_\" + side + \".SUB.fastq\")\n",
    "                    temp.append(\"mv \" + basename + \"_\" + side + \".SUB.fastq.gz \" + basename + \"_\" + side + \".fastq.gz\")\n",
    "        # change names\n",
    "        temp.append(\"mv \" + basename + \"_1.fastq.gz \" + basename + \".1.fastq.gz\")\n",
    "        temp.append(\"mv \" + basename + \"_2.fastq.gz \" + basename + \".2.fastq.gz\")\n",
    "    \n",
    "    elif \"groups\" in subtable[\"read_path\"].to_list()[0]:\n",
    "        \n",
    "        read_path = subtable[\"read_path\"].to_list()[0]\n",
    "        if \"gz\" not in read_path:\n",
    "            read_path += \".gz\"\n",
    "        temp.append(\"cp \" + read_path + \" \" + rootdir + \"mapping/\" + runid + \".1.fastq.gz\")\n",
    "        temp.append(\"cp \" + read_path.replace(\".1.fa\", \".2.fa\") + \" \" + \\\n",
    "            rootdir + \"mapping/\" + runid + \".2.fastq.gz\")\n",
    "    \n",
    "    else: continue\n",
    "    \n",
    "    # filter reads for host\n",
    "    host = subtable[\"host\"].tolist()[0]\n",
    "    if host in ref_genomes:\n",
    "        temp.append(\"echo Filtering out %s reads... >> %s\" %(host, basename + \".log\"))\n",
    "        temp.append(\"bbduk.sh threads=48 qhdist=1 in1=%s in2=%s out1=%s out2=%s ref=%s\" %(\n",
    "            forward, reverse, forward.replace(runid, runid + \"_decontam\"), reverse.replace(runid, runid + \"_decontam\"), ref_genomes[host]))\n",
    "        readname = basename + \"_decontam\"\n",
    "    \n",
    "    # quality filter reads\n",
    "    temp.append(\"echo Quality filtering reads... >> %s\" %(basename + \".log\"))\n",
    "    temp.append(\"process_reads_bbmap.rb --basename \" + readname + \" -c -z -p 48\")\n",
    "    \n",
    "    # gather genomes\n",
    "    genomes = list(subtable[\"newname\"])\n",
    "    catfile = rootdir + \"mapping/genomes_\" + runid + \".fna\"\n",
    "    temp.append(\"echo Combining %s into %s... >> %s\" %(\",\".join(genomes), os.path.basename(catfile), basename + \".log\"))\n",
    "    temp.append(\"cat %s > %s\" %(\" \".join([rootdir + \"genome/\" + genome + \".fna\" for genome in genomes]), catfile))\n",
    "    \n",
    "    #build index, map\n",
    "    temp.append(\"echo Mapping read files against %s... >> %s\" %(os.path.basename(catfile), basename + \".log\"))\n",
    "    temp.append(\"bowtie2-build \" + catfile + \" \" + catfile.replace(\".fna\", \"\"))\n",
    "    temp.append(\"bowtie2 -p 48 -x \" + catfile.replace(\".fna\", \"\") + \" -1 \" + \\\n",
    "        readname + \"_trim_clean.PE.1.fastq.gz -2 \" + readname + \"_trim_clean.PE.2.fastq.gz 2>> \" + basename + \".log\" + \\\n",
    "        \" | shrinksam | samtools view -S -b > \" + basename + \".bam\")\n",
    "    temp.append(\"samtools sort --threads 48 \" + basename + \".bam > \" + basename + \".sorted.bam\")\n",
    "    temp.append(\"samtools index -@ 48 \" + basename + \".sorted.bam\")\n",
    "\n",
    "    # delete read and build files\n",
    "    temp.append(\"rm \" + readname + \".[12].fastq* \" + readname + \"_trim.[12].fastq.* \" + \\\n",
    "        readname + \"*SR.fastq* \" + rootdir + \"mapping/genomes* \" + readname + \"_trim_clean.[12].fastq.*\")\n",
    "    temp.append(\"pigz -p 48 \" + readname + \"_trim_clean.PE.fa\")\n",
    "    temp.append(\"Success for %s... >> %s\" %(runid, basename + \".log\"))\n",
    "\n",
    "    # if not already run, store\n",
    "    try:\n",
    "        size = os.stat(basename + \".bam\").st_size\n",
    "        if size == 0:\n",
    "            callsets.append(temp)\n",
    "    except FileNotFoundError:\n",
    "        callsets.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to multiple wrappers\n",
    "n = math.ceil(len(callsets)/1)\n",
    "for i in range(0, len(callsets),n):\n",
    "    with open(rootdir + \"mapping/prc\" + \\\n",
    "        str(int(i/n)+1) + \".sh\", \"w\") as wrapper:\n",
    "        for callset in callsets[i:i + n]:\n",
    "            for call in callset:\n",
    "                wrapper.write(call + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for item in $(ls | grep prc); do sbatch -J $item --wrap \"$(pwd)/$item\"; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for fail + rerun\n",
    "count=0\n",
    "\n",
    "for bam in glob.glob(rootdir + \"mapping/*[0-9].bam\"):\n",
    "    # check file size\n",
    "    if os.stat(bam).st_size == 0:\n",
    "        sra = os.path.basename(bam).replace(\".bam\",\"\")\n",
    "        count+=1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get non zero sorted bams\n",
    "sorted_bams = [item for item in glob.glob(rootdir + \"/mapping/*sorted.bam\") if os.stat(item).st_size != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverm = \"coverm genome --genome-fasta-directory \" + rootdir + \"/genomes/ -x fna --min-read-percent-identity 0.99 \" + \\\n",
    "    \"--min-covered-fraction 0 --no-zeros --output-format sparse -b \" + \" \".join(sorted_bams) + \" -m\" + \\\n",
    "    \" count mean covered_fraction length rpkm > \" + rootdir + \"/coverage_table.tsv\"\n",
    "\n",
    "out = open(rootdir + \"/scripts/coverm.sh\", \"w\")\n",
    "out.write(coverm)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run manually in shell from `/scripts/` dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtable = pd.read_csv(rootdir + \"coverage_table.tsv\", sep=\"\\t\")\n",
    "rdf = pd.read_csv(rootdir + \"metadata/rdf.tsv\", sep=\"\\t\")\n",
    "covtable = covtable[covtable[\"Read Count\"]>0].sort_values(\"Genome\")\n",
    "covtable[\"run_id\"] = covtable[\"Sample\"].apply(lambda x: x.split(\".\")[0])\n",
    "bin2tax = {row[\"newname\"]: row[\"taxcat\"] for key, row in mc.iterrows()}\n",
    "covtable[\"taxcat\"] = covtable[\"Genome\"].map(bin2tax)\n",
    "# filter out bad run ids\n",
    "valid = rdf[\"run_id\"].unique()\n",
    "covtable = covtable[covtable[\"run_id\"].isin(valid)]\n",
    "covtable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in previously computed read counts\n",
    "read_counts = pd.concat(pd.read_csv(item, sep=\"\\t\") for item in glob.glob(rootdir + \"/metadata/read_counts*.tsv\"))\n",
    "read_counts.columns = [\"sample\", \"read_total\"]\n",
    "read_counts[\"run_id\"] = read_counts[\"sample\"].apply(lambda x: x.split(\"_trim_clean.PE.fa.gz\")[0].replace(\"_decontam\", \"\"))\n",
    "\n",
    "# write out those not already computed\n",
    "with open(rootdir + \"metadata/read_files.txt\", \"w\") as out:\n",
    "    for key, row in covtable.drop_duplicates(\"Sample\").iterrows():\n",
    "        sample = row[\"Sample\"].replace(\".sorted\",\"\")\n",
    "        # if not already computed\n",
    "        if sample not in read_counts[\"run_id\"].to_list():\n",
    "            out.write(glob.glob(rootdir + \"mapping/\" + sample + \"_*PE.fa.gz\")[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run `/scripts/computeReadCounts.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in that + env data\n",
    "meta = mc[[\"newname\", \"env_broad\", \"env_narrow\"]].merge(curatedm[[\"newname\", \"host\"]], how=\"left\")\n",
    "covmerge = covtable.merge(meta, how=\"left\", left_on=\"Genome\", right_on=\"newname\")\n",
    "covmerge = covmerge.merge(read_counts[[\"run_id\", \"read_total\"]])\n",
    "covmerge = covmerge[[\"run_id\",\"env_broad\", \"env_narrow\", \"host\", \"Genome\", \"taxcat\", \"Read Count\", \"Covered Fraction\", \"read_total\", \"RPKM\"]]\n",
    "# compute relative abundance\n",
    "covmerge[\"perc_reads\"] = covmerge.apply(lambda x: (x[\"Read Count\"]/float(x[\"read_total\"]))*100 if x[\"read_total\"] != \"None\" else 0, axis=1)\n",
    "covmerge[\"log_perc_reads\"] = covmerge[\"perc_reads\"].apply(lambda x: math.log10(x) if x != 0 else 0)\n",
    "covmerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select max per genome\n",
    "covsub = covmerge.sort_values(\"log_perc_reads\", ascending=False).drop_duplicates(\"Genome\")\n",
    "# implement cov frac min\n",
    "covsub = covsub[covsub[\"Covered Fraction\"]>=0.1]\n",
    "covsub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# across env_broad\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(3,7))\n",
    "sns.boxplot(\"perc_reads\",\"env_broad\", hue=\"taxcat\", color=\"lightgrey\", linewidth=0.5, data=covsub, fliersize=0)\n",
    "sns.stripplot(\"perc_reads\", \"env_broad\", hue=\"taxcat\", data=covsub, dodge=True, size=3)\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid('on', which='major', axis='x' )\n",
    "plt.xlabel(\"max percent reads\")\n",
    "plt.ylabel(\"\")\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.savefig(rootdir + \"figures/env_broad.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# among animal assc\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(3,4))\n",
    "subset = covsub[covsub[\"env_narrow\"].isin([\"animal gut\", \"animal oral\", \"human gut\", \"human oral\"])].sort_values(\"env_narrow\")\n",
    "sns.boxplot(\"perc_reads\",\"env_narrow\", hue=\"taxcat\", color=\"lightgrey\", linewidth=0.5, data=subset, fliersize=0)\n",
    "sns.stripplot(\"perc_reads\", \"env_narrow\", hue=\"taxcat\", data=subset, dodge=True, size=3)\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid('on', which='major', axis='x' )\n",
    "plt.xlabel(\"max percent reads\")\n",
    "plt.ylabel(\"\")\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.savefig(rootdir + \"figures/env_narrow.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### supp table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = covsub\n",
    "s2[\"accession\"] = s2[\"run_id\"].apply(lambda x: x if \"RR\" in x else \"None\")\n",
    "s2 = s2[[\"Genome\", \"taxcat\", \"env_broad\", \"env_narrow\", \"run_id\", \"accession\", \"Read Count\", \"read_total\", \"perc_reads\", \"log_perc_reads\", \"Covered Fraction\"]]\n",
    "s2.columns = [\"new_bin_name\", \"lineage\", \"habitat_broad\", \"habitat_narrow\", \"run_id\", \"accession\", \"mapped_read_count\", \"sample_read_total\", \"mapped_read_percent\", \"log_mapped_read_percent\", \"fraction_genome_covered\"]\n",
    "s2.sort_values([\"lineage\", \"habitat_broad\", \"habitat_narrow\", \"run_id\"]).to_csv(rootdir + \"metadata/supp_table_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"assembly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapperize(calls, parts, out, dtype):\n",
    "    \n",
    "    n = math.ceil(len(calls)/parts)\n",
    "    for i in range(0, len(calls),n):\n",
    "        with open(out + \\\n",
    "            str(int(i/n)+1) + \".sh\", \"w\") as wrapper:\n",
    "            for call in calls[i:i + n]:\n",
    "                if dtype == \"str\":\n",
    "                    wrapper.write(call + \"\\n\")\n",
    "                elif dtype == \"list\":\n",
    "                    for subcall in call:\n",
    "                        wrapper.write(subcall + \"\\n\")\n",
    "    print(\"chmod +x \" + out + \"*\")\n",
    "    print('for item in $(ls ' + out + '*); do sbatch --wrap \"$item\"; done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assemble reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runsub = covsub[[\"run_id\", \"Genome\"]].merge(rdf[[\"run_id\", \"assembly_path\"]])\n",
    "# add back in read locations once computed\n",
    "runsub[\"read_path\"] = runsub[\"run_id\"].apply(lambda x: glob.glob(rootdir + \\\n",
    "    \"/mapping/\" + x + \"_*PE.fa.gz\")[0] if glob.glob(rootdir + \"/mapping/\" + x + \"_*PE.fa.gz\") != [] else \"None\")\n",
    "runsub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megas = []\n",
    "eligible = []\n",
    "\n",
    "for key, row in runsub[[\"run_id\", \"read_path\", \"assembly_path\"]].drop_duplicates().iterrows():\n",
    "    \n",
    "    if (row[\"read_path\"] != \"None\") & (row[\"assembly_path\"]==\"None\") :\n",
    "        \n",
    "        base = os.path.basename(row[\"run_id\"])\n",
    "        eligible.append(base)\n",
    "        \n",
    "        # if not already successfully assembled\n",
    "        if not os.path.isfile(rootdir + \"/assembly/\" + base + \"/done\"):\n",
    "            megas.append(\"megahit --verbose --min-contig-len 1000 -t 48 --12 \" + \\\n",
    "                row[\"read_path\"]+ \" -o \" + rootdir + \"/assembly/\" + base + \"/ --out-prefix \" + base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any that need to be deleted?\n",
    "previously_computed = [path.split(\"/\")[-2] for path in glob.glob(rootdir + \"assembly/*/\")]\n",
    "to_delete = list(set(previously_computed) - set(eligible))\n",
    "\n",
    "print(to_delete)\n",
    "\n",
    "for item in to_delete:\n",
    "    call = \"rm -r \" + rootdir + \"assembly/\" + item\n",
    "    #sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapperize(megas, 25, rootdir + \"assembly/assem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename SRA assembly scafs\n",
    "\n",
    "for assembly_dir in glob.glob(rootdir + \"assembly/*/\"):\n",
    "    \n",
    "    name = assembly_dir.split(\"/\")[-2]\n",
    "    # only if assembly done\n",
    "    if os.path.isfile(assembly_dir + \"done\"):\n",
    "        contigs = glob.glob(assembly_dir + \"*contigs.fa*\")[0]\n",
    "        renamed = contigs.replace(\"contigs\", \"renamed\")\n",
    "        \n",
    "        # if not already renamed\n",
    "        if not os.path.isfile(renamed):\n",
    "            \n",
    "            count=1\n",
    "            with open(renamed,\"w\") as new_assembly:\n",
    "                for record in SeqIO.parse(open(contigs), \"fasta\"):\n",
    "                    new_assembly.write(\">\" + name + \"_scaffold_\" + \\\n",
    "                        str(count) + \"\\n\" + str(record.seq) + \"\\n\")\n",
    "                    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename GGK assembly scafs\n",
    "\n",
    "for key, row in runsub[runsub[\"assembly_path\"]!=\"None\"].drop_duplicates([\"run_id\", \"assembly_path\"]).iterrows():\n",
    "    \n",
    "    # if not already done\n",
    "    if not os.path.isdir(rootdir + \"assembly/\" + row[\"run_id\"]):\n",
    "        \n",
    "        # find actual assembly\n",
    "        clean_path = \"/\".join(row[\"assembly_path\"].split(\"/\")[:-1])\n",
    "        ass_path = glob.glob(clean_path + \"/*min[0-9]000.fa\")\n",
    "        \n",
    "        if len(ass_path) == 1:\n",
    "\n",
    "            count=1\n",
    "            cmdir(rootdir + \"assembly/\" + row[\"run_id\"])\n",
    "            with open(rootdir + \"assembly/\" + row[\"run_id\"] + \"/\" + row[\"run_id\"] + \".renamed.fa\", \"w\") as newass:\n",
    "                for record in SeqIO.parse(open(ass_path[0]), \"fasta\"):\n",
    "                    newass.write(\">\" + row[\"run_id\"] + \"_scaffold_\" + \\\n",
    "                        str(count) + \"\\n\" + str(record.seq) + \"\\n\")\n",
    "                    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back assembly paths for ALL run_ids\n",
    "runsub[\"curated_assembly_path\"] = runsub[\"run_id\"].apply(lambda x: glob.glob(rootdir + \\\n",
    "    \"/assembly/*/\" + x + \".renamed.fa\")[0] if glob.glob(rootdir + \"/assembly/*/\" + x + \".renamed.fa\") != [] else \"None\")\n",
    "runsub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which need to be re-assembled later?\n",
    "runsub[runsub[\"curated_assembly_path\"]==\"None\"][\"run_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output table for use elsewhere\n",
    "temp = runsub[[\"Genome\", \"run_id\", \"read_path\", \"curated_assembly_path\"]].drop_duplicates()\n",
    "temp.columns = [\"genome\",\"run_id\", \"read_path\", \"assembly_path\"]\n",
    "temp.to_csv(rootdir + \"metadata/curated_run_info.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# marker gene analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"graftm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run graftm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gpkgs\n",
    "s3 = rootdir + \"other/S3.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafts = []\n",
    "\n",
    "for assembly in glob.glob(rootdir + \"/assembly/*/*.renamed.fa\"):\n",
    "    \n",
    "    runid = os.path.basename(assembly).split(\".\")[0]\n",
    "    # if not already computed\n",
    "    if not os.path.isdir(rootdir + \"graftm/\" + runid):\n",
    "        call = \"graftM graft --threads 48 --forward \" + \\\n",
    "            assembly + \" --input_sequence_type nucleotide --graftm_package \" + s3 + \\\n",
    "            \" --output_directory \" + rootdir + \"graftm/\" + runid\n",
    "        grafts.append(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapperize(grafts, 4, rootdir + \"graftm/grm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from RpxSuite - https://github.com/alexcritschristoph/\n",
    "    ##RPxSuite/blob/master/rpXsuite/RPxSuite.py\n",
    "\n",
    "def parse_usearch_clustering(loc):\n",
    "\n",
    "    dtypes = {0:'category', 1:'category', 2:np.int32, 8:'object'}\n",
    "    ucols = [0,1,2,8]\n",
    "    Rdb = pd.read_csv(loc, header=None, usecols=ucols,\\\n",
    "            dtype=dtypes, sep='\\t')\n",
    "    table = defaultdict(list)\n",
    "\n",
    "    # Find the centroids\n",
    "    sdb  = Rdb[Rdb[0] == 'S']\n",
    "    shdb = Rdb[Rdb[0].isin(['H', 'S'])]\n",
    "    for centroid, cdb in sdb.groupby(1):\n",
    "        cent = cdb[8].tolist()[0].split()[0]\n",
    "        db = shdb[shdb[1] == centroid]\n",
    "\n",
    "        for seq in db[8].tolist():\n",
    "            table['cluster'].append(int(centroid))\n",
    "            table['members'].append(len(db))\n",
    "            table['sequence'].append(seq.split()[0])\n",
    "            table['centroid'].append(cent)\n",
    "\n",
    "    return pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cresults = {}\n",
    "\n",
    "for graftdir in glob.glob(rootdir + \"graftm/*/*/\"):\n",
    "        \n",
    "    name = graftdir.split(\"/\")[-2].replace(\".renamed\", \"\")\n",
    "    # find files\n",
    "    taxinfo = glob.glob(graftdir + \"*renamed_read_tax.tsv\")[0]\n",
    "    \n",
    "    # only proceed if results\n",
    "    if os.stat(taxinfo).st_size != 0:\n",
    "        \n",
    "        full_fasta = glob.glob(graftdir + \"*renamed_orf.fa\")[0]\n",
    "        # perform clustering\n",
    "        call = \"usearch -cluster_fast \" + full_fasta + \" -sort length -id 0.99 \" + \\\n",
    "            \"-centroids \" + full_fasta.replace(\"renamed_orf\", \"centroids\") + \\\n",
    "            \" -uc \" + taxinfo.replace(\"renamed_read_tax.tsv\", \"clusters\")\n",
    "        sp.call(call, shell=True)\n",
    "\n",
    "        # process clustering\n",
    "        cluster_results = parse_usearch_clustering(taxinfo.replace(\"renamed_read_tax.tsv\", \"clusters\"))\n",
    "        # merge tax calls\n",
    "        cluster_results = cluster_results.merge(pd.read_csv(taxinfo, sep=\"\\t\", names=[\"sequence\", \"taxstring\"]))\n",
    "        # save out\n",
    "        cresults[name] = cluster_results[[\"cluster\", \"members\", \"centroid\", \"taxstring\"]].drop_duplicates()\n",
    "    \n",
    "    else: print(\"%s had no marker gene hits!\" %(name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = defaultdict(list)\n",
    "\n",
    "for key, results in cresults.items():\n",
    "    \n",
    "    cluster_counts[\"run_id\"].append(key)\n",
    "    cluster_counts[\"uniq_actino\"].append(len(results[results[\"taxstring\"].str.contains(\"p__Actinobacteriota\")]))\n",
    "    cluster_counts[\"uniq_sacchari\"].append(len(results[results[\"taxstring\"].str.contains(\"c__Saccharimonadia\")]))\n",
    "\n",
    "countsdf = pd.DataFrame(cluster_counts)\n",
    "countsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `sum-bp  assembly/*/*renamed.fa > metadata/sumbp_results.txt` in the terminal to get assembly totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in sum-bp results\n",
    "sums = {line.split(\" \")[0].split(\".\")[0]: int(line.split(\" \")[1].strip().replace(\",\", \"\")) \n",
    "    for line in open(rootdir + \"metadata/sumbp_results.txt\").readlines()}\n",
    "\n",
    "countsdf[\"sumbp\"] = countsdf[\"run_id\"].map(sums)\n",
    "countsdf[\"megabases\"] = countsdf[\"sumbp\"].apply(lambda x: x/float(1e6))\n",
    "\n",
    "# compute per Mb counts\n",
    "for tax in [\"actino\", \"sacchari\"]:\n",
    "    countsdf[tax + \"_permb\"] = countsdf.apply(lambda x: x[\"uniq_\" + tax]/float(x[\"megabases\"]), axis=1)\n",
    "\n",
    "# finally merge in environments\n",
    "countsdf = countsdf.merge(covmerge[[\"run_id\", \"env_broad\", \"env_narrow\"]], how=\"left\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many with no saccharis assembled?\n",
    "countsdf[(countsdf[\"sacchari_permb\"]==0) | (countsdf[\"actino_permb\"]==0)][\"env_narrow\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "nozed = countsdf[(countsdf[\"actino_permb\"]>0) & (countsdf[\"sacchari_permb\"]>0)]\n",
    "x = np.linspace(0, 10, 1000)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.scatterplot(\"actino_permb\", \"sacchari_permb\", data=nozed, hue=\"env_broad\", palette=env2color, legend=None)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(10e-4,0)\n",
    "plt.ylim(10e-4, 0)\n",
    "plt.xlabel(\"normalized species richness (Actinobacteria)\")\n",
    "plt.ylabel(\"normalized species richness (Saccharibacteria)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nozed[\"ratio\"] = nozed.apply(lambda x: x[\"uniq_sacchari\"]/x[\"uniq_actino\"], axis=1)\n",
    "order = nozed.groupby(\"env_broad\", as_index=False).aggregate({\"ratio\":\"mean\"}).sort_values(\"ratio\", ascending=False)[\"env_broad\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(\"ratio\", \"env_broad\",color=\"white\", order=order, linewidth=0.5, data=nozed, width=0.6, fliersize=0)\n",
    "sns.stripplot(\"ratio\", \"env_broad\", hue=\"env_broad\", order=order, palette=env2color, linewidth=0.1, data=nozed, size=4)\n",
    "plt.xticks(rotation=40, horizontalalignment=\"right\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"richness ratio\")\n",
    "plt.axvline(1, ls=\"--\", color=\"grey\")\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.xscale(\"log\")\n",
    "plt.savefig(rootdir + \"figures/ratio.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countsdf[\"missing\"] = countsdf.apply(lambda x: 1 if ((x[\"uniq_actino\"]==0)) else 0, axis=1)\n",
    "countsdf[\"present\"] = countsdf.apply(lambda x: 1 if ((x[\"uniq_actino\"]!=0)) else 0, axis=1)\n",
    "countgb = countsdf.groupby(\"env_broad\", as_index=False).aggregate({\"missing\":\"sum\", \"present\": \"sum\"})\n",
    "\n",
    "for cat in [\"missing\", \"present\"]:\n",
    "    countgb[cat + \"_perc\"] = countgb.apply(lambda x: int(x[cat])/float(int(x[\"missing\"]) + int(x[\"present\"]))*100, axis=1)\n",
    "\n",
    "countgb.index = countgb[\"env_broad\"]\n",
    "countgb = countgb.drop([\"env_broad\", \"missing\", \"present\"], axis=1)[[\"present_perc\", \"missing_perc\"]]\n",
    "order.reverse()\n",
    "countgb.loc[order,:].plot.barh(color=[\"darkgrey\", \"white\"], stacked=True, edgecolor=\"grey\", width=0.6, linewidth=0.5, figsize=(1,5))\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.ylabel(\" \")\n",
    "plt.xlabel(\"%\")\n",
    "plt.tick_params(left=False)\n",
    "sns.despine(left=True)\n",
    "plt.savefig(rootdir + \"figures/bar.svg\", format=\"svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cmdir(rootdir + \"trees\")### across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"trees\")\n",
    "all_results = pd.concat(cresults.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all markers to use as db\n",
    "with open(rootdir + \"trees/all_rps3.faa\", \"w\") as catfile:\n",
    "    for graftdir in glob.glob(rootdir + \"graftm/*/*/\"):\n",
    "        if os.stat(glob.glob(graftdir + \"*renamed_read_tax.tsv\")[0]).st_size != 0:\n",
    "            for record in SeqIO.parse(open(glob.glob(graftdir + \"*renamed_orf.fa\")[0]), \"fasta\"):\n",
    "                # only copy over if already a within-sample centroid\n",
    "                if record.description in all_results[\"centroid\"].to_list():\n",
    "                    catfile.write(\">%s\\n%s\\n\" %(record.description.split(\" \")[0], str(record.seq)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster across samples\n",
    "call = \"usearch -cluster_fast \" + rootdir + \"/trees/all_rps3.faa -sort length -id 0.99 \" + \\\n",
    "    \"-centroids \" + rootdir + \"/trees/all_rps3.centroids\" + \\\n",
    "    \" -uc \" + rootdir + \"/trees/all_rps3.clusters\"\n",
    "sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process clustering\n",
    "cluster_results = parse_usearch_clustering(rootdir + \"/trees/all_rps3.clusters\")\n",
    "# merge back taxstring\n",
    "cluster_results = cluster_results.merge(all_results[[\"centroid\", \"taxstring\"]], on=\"centroid\", how=\"left\").fillna(\"None\")\n",
    "cluster_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get experimental tm7-host pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skbio.io\n",
    "from io import StringIO\n",
    "cmdir(rootdir + \"reference_genomes/cpr-actino\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in compiled metadata\n",
    "hostdf = pd.read_csv(rootdir + \"/metadata/crossenv_sacchari_hosts.tsv\", sep=\"\\t\")\n",
    "hostdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the genomes\n",
    "sacc = [item for item in list(hostdf[\"sacchari_accession\"].unique()) if item != \"None\"]\n",
    "aacc = [item for item in list(hostdf[\"host_accession\"].unique()) if item != \"None\"]\n",
    "\" OR \".join(sacc + aacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab from NCBI and move into dir, unzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join it up\n",
    "hostdf[\"sfilename\"] = hostdf[\"sacchari_accession\"].apply(lambda x: [item for item in glob.glob(rootdir + \"reference_genomes/cpr-actino/*\") if x in item][0] if x!= \"None\" else \"None\")\n",
    "hostdf[\"hfilename\"] = hostdf[\"host_accession\"].apply(lambda x: [item for item in glob.glob(rootdir + \"reference_genomes/cpr-actino/*\") if x in item][0] if x!= \"None\" else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat *fna > all_references.fna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract s3\n",
    "call = \"graftM graft --threads 16 --forward \" + \\\n",
    "        rootdir + \"reference_genomes/cpr-actino/all_references.fna\" + \\\n",
    "        \" --input_sequence_type nucleotide --graftm_package \" + s3 + \\\n",
    "        \" --output_directory \" + rootdir + \"reference_genomes/cpr-actino/all_references\"\n",
    "print(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"/scripts/blast_markers.sh\", \"w\") as out:\n",
    "    \n",
    "    # make blast db\n",
    "    makedb = \"makeblastdb -in \" + rootdir + \"trees/all_rps3.centroids -dbtype prot\"\n",
    "    # perform blast\n",
    "    blast = \"blastp -db %s -query %s -out %s -evalue 1e-3 -max_target_seqs 10 -num_threads 16 -sorthits 3 -outfmt 6\" %(rootdir + \"trees/all_rps3.centroids\", rootdir + \"reference_genomes/cpr-actino/all_references/all_references/all_references_orf.fa\", rootdir + \"trees/reference.blast.results\")\n",
    "    out.write(makedb + \"\\n\" + blast + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ref seq lens\n",
    "ref_lens = {record.description.split(\" \")[0]:len(record.seq) \n",
    "    for record in SeqIO.parse(open(rootdir + \"reference_genomes/cpr-actino/all_references/all_references/all_references_orf.fa\"), \"fasta\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bresults = skbio.io.read(rootdir + \"/trees/reference.blast.results\", format=\"blast+6\", into=pd.DataFrame, default_columns=True)\n",
    "# compute coverage\n",
    "bresults[\"qlen\"] = bresults[\"qseqid\"].map(ref_lens)\n",
    "bresults[\"qcov\"] = bresults.apply(lambda x: (x[\"qend\"]-x[\"qstart\"])/x[\"qlen\"], axis=1)\n",
    "# choose best hits for each\n",
    "bresults = bresults.sort_values([\"bitscore\", \"qcov\"], ascending=[False,False]).drop_duplicates(\"qseqid\")\n",
    "# make scaf2bin\n",
    "bresults[\"scaffold\"] = bresults[\"qseqid\"].apply(lambda x: x.split(\"_\")[0])\n",
    "# scaffold 2 bin\n",
    "refscaf2bin = {}\n",
    "for file in glob.glob(rootdir + \"reference_genomes/cpr-actino/GCA*fna\"):\n",
    "    for record in SeqIO.parse(open(file), \"fasta\"):\n",
    "        refscaf2bin[record.description.split(\" \")[0]] = file\n",
    "bresults[\"filename\"] = bresults[\"scaffold\"].map(refscaf2bin)\n",
    "bresults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter matches at 99% PID (to match clustering) and 95% query coverage\n",
    "filename2marker = {}\n",
    "\n",
    "for key, row in bresults.iterrows():\n",
    "    \n",
    "    if (row[\"pident\"]>=99) and (row[\"qcov\"]>=0.95):\n",
    "        filename2marker[row[\"filename\"]] = row[\"sseqid\"]\n",
    "    else: #maintain original name\n",
    "        filename2marker[row[\"filename\"]] = row[\"qseqid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these edges may or may not exist already in the dataframe,\n",
    "# so in case they have not been oberved, reconstruct auxiliary df\n",
    "aux = defaultdict(list)\n",
    "\n",
    "for key, row in hostdf.iterrows():\n",
    "    \n",
    "    try:\n",
    "        smarker = filename2marker[row[\"sfilename\"]]\n",
    "        hmarker = filename2marker[row[\"hfilename\"]]\n",
    "        aux[\"scentroid\"].append(smarker)\n",
    "        aux[\"acentroid\"].append(hmarker)\n",
    "        \n",
    "    except:\n",
    "        print(\"%s with %s is missing.\" %(row[\"sacchari_name\"], row[\"host_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### supp table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequences\n",
    "seq_dict = {record.description.split(\" \")[0]: str(record.seq) for record in SeqIO.parse(open(rootdir + \"trees/all_rps3.faa\"), \"fasta\")}\n",
    "ref_dict = {record.description.split(\" \")[0]: str(record.seq) for record in SeqIO.parse(open(rootdir + \"reference_genomes/cpr-actino/all_references/all_references/all_references_orf.fa\"), \"fasta\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bresults_sub = bresults[(bresults[\"pident\"]>=99) & (bresults[\"qcov\"]>=0.95)][[\"filename\", \"sseqid\", \"pident\", \"qcov\"]]\n",
    "bresults_sub[\"pident\"] = bresults_sub[\"pident\"].apply(lambda x: round(x, 5))\n",
    "bresults_sub[\"qcov\"] = bresults_sub[\"qcov\"].apply(lambda x: round(x, 5))\n",
    "bresults_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostdf[\"saccharibacteria_marker\"] = hostdf[\"sfilename\"].map(filename2marker).fillna(\"None\")\n",
    "hostdf[\"host_marker\"] = hostdf[\"hfilename\"].map(filename2marker).fillna(\"None\")\n",
    "s3 = hostdf.merge(bresults_sub[[\"filename\", \"pident\", \"qcov\"]], how=\"left\", left_on=\"sfilename\", right_on=\"filename\")\n",
    "s3 = s3.merge(bresults_sub[[\"filename\", \"pident\", \"qcov\"]], how=\"left\", left_on=\"hfilename\", right_on=\"filename\")\n",
    "s3 = s3.drop([\"sfilename\", \"hfilename\", \"notes\",\"filename_x\", \"filename_y\"], axis=1)\n",
    "s3.columns = [\"saccharibacteria_name\", \"saccharibacteria_accession\", \"host_name\", \"host_accession\", \"reference\", \"saccharibacteria_s3_marker\", \"host_s3_marker\", \"saccharibacteria_marker_pid\", \"saccharibacteria_marker_cov\", \"host_marker_pid\", \"host_marker_cov\"]\n",
    "# remove those with no S3 or no ref genome\n",
    "s3 = s3[(s3[\"saccharibacteria_s3_marker\"]!='None') & (s3[\"host_s3_marker\"]!=\"None\")]\n",
    "# now add sequences\n",
    "s3[\"saccharibacteria_s3_seq\"] = s3[\"saccharibacteria_s3_marker\"].apply(lambda x: seq_dict[x] if x in seq_dict.keys() else ref_dict[x])\n",
    "s3[\"host_s3_seq\"] = s3[\"host_s3_marker\"].apply(lambda x: seq_dict[x] if x in seq_dict.keys() else ref_dict[x])\n",
    "s3 = s3[['saccharibacteria_name', 'saccharibacteria_accession', 'host_name', 'host_accession', 'reference',\n",
    "        'saccharibacteria_s3_marker', 'saccharibacteria_marker_pid', 'saccharibacteria_marker_cov', 'saccharibacteria_s3_seq',\n",
    "        'host_s3_marker', 'host_marker_pid', 'host_marker_cov', \"host_s3_seq\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.fillna(\"None\").to_csv(rootdir + \"metadata/supp_table_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tree-building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hmm(result_table):\n",
    "    \n",
    "    temp = {}\n",
    "    count = 0\n",
    "    # parse each result file using searchio\n",
    "    for result in SearchIO.parse(result_table, \"hmmer3-tab\"):\n",
    "        for item in result.hits:\n",
    "            temp[count] = {\"gene\": item.id, \"score\": item.bitscore, \"eval\": item.evalue}\n",
    "            count += 1\n",
    "            \n",
    "    return(pd.DataFrame.from_dict(temp, orient=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build unmatched ref seqs\n",
    "unmatched = bresults[(bresults[\"pident\"]<99) | (bresults[\"qcov\"]<0.95)]\n",
    "unmatcheds = unmatched[bresults[\"filename\"].isin(hostdf[\"sfilename\"])][\"qseqid\"].to_list()\n",
    "unmatcheda = unmatched[bresults[\"filename\"].isin(hostdf[\"hfilename\"])][\"qseqid\"].to_list()\n",
    "\n",
    "with open(rootdir + \"trees/saccharimonadia_unmatched.faa\", \"w\") as out:\n",
    "    for record in SeqIO.parse(open(rootdir + \"reference_genomes/cpr-actino/all_references/all_references/all_references_orf.fa\"), \"fasta\"):\n",
    "        if record.description.split(\" \")[0] in unmatcheds:\n",
    "            out.write(\">\" + record.description.split(\" \")[0] + \"\\n\" + str(record.seq) + \"\\n\")\n",
    "            \n",
    "with open(rootdir + \"trees/actinobacteriota_unmatched.faa\", \"w\") as out:\n",
    "    for record in SeqIO.parse(open(rootdir + \"reference_genomes/cpr-actino/all_references/all_references/all_references_orf.fa\"), \"fasta\"):\n",
    "        if record.description.split(\" \")[0] in unmatcheda:\n",
    "            out.write(\">\" + record.description.split(\" \")[0] + \"\\n\" + str(record.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build bac175 outgroup\n",
    "call = \"hmmsearch --cut_nc --tblout \" + rootdir + \"trees/bac175.s3.results \" + \\\n",
    "    rootdir + \"other/TIGR01009.HMM \" + rootdir + \"reference_genomes/bac175/Bacteria175.cleaned.faa\"\n",
    "sp.call(call, shell=True)\n",
    "\n",
    "with open(rootdir + \"trees/bac175.s3.txt\", \"w\") as out:\n",
    "    for key, row in parse_hmm(rootdir + \"trees/bac175.s3.results\").iterrows():\n",
    "        # remove actinos\n",
    "        if row[\"gene\"] not in []:\n",
    "             out.write(row[\"gene\"] + \"\\n\")\n",
    "\n",
    "call = \"pullseq -n \" + rootdir + \"trees/bac175.s3.txt -i \" + \\\n",
    "    rootdir + \"reference_genomes/bac175/Bacteria175.cleaned.faa > \" + rootdir + \"trees/bac175.s3.faa\"\n",
    "sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = open(rootdir + \"trees/wrapper.sh\", \"w\")\n",
    "\n",
    "for term in [\"p__Actinobacteriota\", \"c__Saccharimonadia\"]:\n",
    "    \n",
    "    sub = cluster_results[(cluster_results[\"taxstring\"].str.contains(term))]\n",
    "    names_file = rootdir + \"trees/\" + term.split(\"_\")[2].lower() + \"_rps3.txt\"\n",
    "    \n",
    "    with open(names_file, \"w\") as outfile:\n",
    "        for key, row in sub.iterrows():\n",
    "            outfile.write(row[\"centroid\"]+'\\n')\n",
    "    \n",
    "    # create fasta file\n",
    "    pull = \"pullseq -n \" + names_file + \" -i \" + \\\n",
    "        rootdir + \"trees/all_rps3.faa > \" + names_file.replace(\".txt\", \".faa\")\n",
    "    \n",
    "    # add out group and unmatched refs\n",
    "    cat = \"cat \" + rootdir + \"trees/bac175.s3.faa \" + names_file.replace(\"rps3.txt\", \"unmatched.faa\") + \\\n",
    "        \" \" + names_file.replace(\".txt\", \".faa\") + \" > \" + names_file.replace(\".txt\", \".cat.faa\")\n",
    "    \n",
    "    # align + trim\n",
    "    align = \"mafft --thread 16 --retree 2 --reorder \" + \\\n",
    "        names_file.replace(\".txt\", \".cat.faa\") + \" > \" + names_file.replace(\".txt\", \".mafft\")\n",
    "    \n",
    "    # trimal call\n",
    "    trim = \"trimal -in \" + names_file.replace(\".txt\", \".mafft\") + \" -out \" + \\\n",
    "        names_file.replace(\".txt\", \".trimal.mafft\") + \" -gt 0.1\"\n",
    "    \n",
    "    # launch tree inference\n",
    "    tree = \"iqtree -s \" + names_file.replace(\".txt\", \".trimal.mafft\") + \\\n",
    "        \" -m TEST -st AA -bb 1000 -nt 10 -pre \" + names_file.replace(\".txt\", \"\")\n",
    "    print(tree)\n",
    "    wrapper.write(pull + \"\\n\" + cat + \"\\n\" + align + \"\\n\" + trim + \"\\n\")\n",
    "    \n",
    "wrapper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out itol\n",
    "env_dict = {row[\"run_id\"]:row[\"env_broad\"] for key, row in covsub.fillna(\"None\").iterrows()}\n",
    "\n",
    "# for unmatched references\n",
    "for item in unmatched[\"qseqid\"].unique():\n",
    "    env_dict[item] = \"animal-associated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in [\"saccharimonadia\", \"actinobacteriota\"]:\n",
    "    with open(rootdir + \"/trees/\" + group + \"_rps3.itol.txt\", \"w\") as itol:\n",
    "        itol.write(\"TREE_COLORS\\nSEPARATOR TAB\\nDATA\\n\")\n",
    "        for leaf in Tree(rootdir + \"/trees/\" + group + \"_rps3.treefile\"):\n",
    "            if \"BAC175\" not in leaf.name:\n",
    "                env = env_dict[leaf.name.split(\"_scaffold\")[0]]\n",
    "                if env != \"None\":\n",
    "                    itol.write(leaf.name + \"\\trange\\t\" + env2color[env] + \\\n",
    "                        \"\\t\" + env_dict[leaf.name.split(\"_scaffold\")[0]] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-occurrence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all comparisons\n",
    "lines = defaultdict(list)\n",
    "\n",
    "for key, table in cresults.items():\n",
    "    \n",
    "    sac = set(table[(table[\"taxstring\"].str.contains(\"c__Saccharimonadia\"))][\"centroid\"].to_list())\n",
    "    act = set(table[(table[\"taxstring\"].str.contains(\"p__Actinobacteriota\"))][\"centroid\"].to_list())\n",
    "    \n",
    "    for s in sac:\n",
    "        for a in act:\n",
    "            lines[\"sample\"].append(key)\n",
    "            lines[\"scluster\"].append(s)\n",
    "            lines[\"acluster\"].append(a)\n",
    "\n",
    "linedf = pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate clustering of clustering\n",
    "#sacchari\n",
    "linedf = linedf.merge(cluster_results[[\"sequence\", \"centroid\"]], how=\"left\", left_on=\"scluster\", right_on=\"sequence\").drop(\"sequence\", axis=1).rename(columns={\"centroid\": \"scentroid\"})\n",
    "# actino\n",
    "linedf = linedf.merge(cluster_results[[\"sequence\", \"centroid\"]], how=\"left\", left_on=\"acluster\", right_on=\"sequence\").drop(\"sequence\", axis=1).rename(columns={\"centroid\": \"acentroid\"})\n",
    "# then aggregate\n",
    "lineagg = linedf.groupby([\"scentroid\", \"acentroid\"], as_index=False).count()[[\"scentroid\", \"acentroid\", \"sample\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO assign order based on phylogeny\n",
    "ssort = {item:i for i, item in enumerate([line.replace(\" \",\"_\").strip() \n",
    "    for line in open(rootdir + \"trees/saccharimonadia_rps3.order.txt\").readlines()])}\n",
    "asort = {item:i for i, item in enumerate([line.replace(\" \",\"_\").strip() \n",
    "    for line in open(rootdir + \"trees/actinobacteriota_rps3.order.txt\").readlines()])}\n",
    "lineagg[\"snum\"] = lineagg[\"scentroid\"].map(ssort)\n",
    "lineagg[\"anum\"] = lineagg[\"acentroid\"].map(asort)\n",
    "# add back habitat info\n",
    "lineagg[\"env_broad\"] = lineagg[\"scentroid\"].apply(lambda x: env_dict[x.split(\"_scaffold\")[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lcs(values):\n",
    "    \n",
    "    max_ranks = max([len(item.split(\";\")) for item in values])\n",
    "    lowest_rank = \"\"\n",
    "    \n",
    "    for i in range(max_ranks):\n",
    "        temp = []\n",
    "        for item in values:\n",
    "            splt = item.split(\";\")\n",
    "            if len(splt) > i:\n",
    "                temp.append(splt[i])\n",
    "            else: temp.append(\"None\")\n",
    "        \n",
    "        unique = set(temp)\n",
    "        if (len(unique)==1) and (list(unique)[0]!=\"None\"):\n",
    "            lowest_rank = list(unique)[0]\n",
    "    \n",
    "    return lowest_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid2tax = {row[\"centroid\"]:row[\"taxstring\"] \n",
    "    for key, row in all_results.iterrows()}\n",
    "lineagg[\"stax\"] = lineagg[\"scentroid\"].map(centroid2tax)\n",
    "lineagg[\"atax\"] = lineagg[\"acentroid\"].map(centroid2tax)\n",
    "edges = {}\n",
    "\n",
    "for scentroid in lineagg[\"scentroid\"].unique():\n",
    "    \n",
    "    table = lineagg[lineagg[\"scentroid\"]==scentroid]\n",
    "    if table[\"env_broad\"].iloc[0] == \"animal-associated\":\n",
    "        lowest =  get_lcs(table[\"atax\"].to_list())\n",
    "        if (\"g__\" in lowest) or (\"o__\" in lowest):\n",
    "            edges[scentroid] = \"exclusive co-occurrence (%s)\" %(lowest.split(\"__\")[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scluster in linedf[\"scluster\"].unique():\n",
    "    \n",
    "    table = linedf[linedf[\"scluster\"]==scluster]\n",
    "    if len(table[\"acentroid\"].unique()) == 1:\n",
    "        edges[table[\"scentroid\"].iloc[0]] = {table[\"acentroid\"].iloc[0]: \"exclusive co-occurrence (sg)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getType(row):\n",
    "    \n",
    "    if row[\"scentroid\"] in edges.keys():\n",
    "        \n",
    "        if type(edges[row[\"scentroid\"]]) == str:\n",
    "            return edges[row[\"scentroid\"]]\n",
    "        \n",
    "        elif row[\"acentroid\"] == list(edges[row[\"scentroid\"]].keys())[0]:\n",
    "            return edges[row[\"scentroid\"]][row[\"acentroid\"]]\n",
    "        \n",
    "        else: return \"co-occurrence\"\n",
    "    \n",
    "    else: return \"co-occurrence\"\n",
    "\n",
    "lineagg[\"type\"] = lineagg.apply(lambda x: getType(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxdf = pd.DataFrame(aux)\n",
    "# merge in snum and anum\n",
    "auxdf[\"snum\"] = auxdf[\"scentroid\"].map(ssort)\n",
    "auxdf[\"anum\"] = auxdf[\"acentroid\"].map(asort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "sspace = len(asort.keys())/(len(ssort.keys()))\n",
    "\n",
    "for key, item in ssort.items():\n",
    "    plt.plot(item*sspace, 0,'bo', color=\"lightgrey\", ms=0.5)\n",
    "for key, item in asort.items():\n",
    "    plt.plot(item, 1,'bo', color=\"lightgrey\", ms=0.5)\n",
    "for key, row in lineagg.iterrows():\n",
    "    if row[\"env_broad\"] == \"animal-associated\":\n",
    "        if row[\"type\"] == \"co-occurrence\":\n",
    "            plt.plot((row[\"snum\"]*sspace, row[\"anum\"]), (0,1), ls=\"-\", alpha=0.1, linewidth=min(1*row[\"sample\"], 8), color=\"lightgrey\",zorder=0)\n",
    "        elif \"sg\" in row[\"type\"]:\n",
    "            plt.plot((row[\"snum\"]*sspace, row[\"anum\"]), (0,1), ls=\"-\", alpha=0.5, linewidth=min(2*row[\"sample\"],8), color=\"lightgreen\",zorder=9)\n",
    "        else:\n",
    "            plt.plot((row[\"snum\"]*sspace, row[\"anum\"]), (0,1), ls=\"-\", alpha=0.5, linewidth=min(2*row[\"sample\"], 8), color=\"pink\",zorder=9)\n",
    "\n",
    "# add in experimental ones\n",
    "for key, row in auxdf.iterrows():\n",
    "    plt.plot((row[\"snum\"]*sspace, row[\"anum\"]), (0,1), ls=\"-\", alpha=0.5, linewidth=3, color=\"lightblue\",zorder=9)#\n",
    "\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlim([-10, max(asort.values())+10])\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.savefig(rootdir + \"figures/occurrence.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### supp table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = lineagg[lineagg[\"type\"]==\"exclusive co-occurrence (sg)\"]\n",
    "lines = []\n",
    "\n",
    "for key, row in sg.iterrows():\n",
    "    sub = linedf[(linedf[\"scentroid\"]==row[\"scentroid\"]) & (linedf[\"acentroid\"]==row[\"acentroid\"])]\n",
    "    subsub = sub[sub[\"sample\"].isin(countsdf[countsdf[\"uniq_actino\"]==1][\"run_id\"].to_list())]\n",
    "    lines.append(subsub)\n",
    "\n",
    "sglines = pd.concat(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = lineagg[lineagg[\"type\"]==\"exclusive co-occurrence (o)\"]\n",
    "\n",
    "lines = []\n",
    "\n",
    "for key, row in o.iterrows():\n",
    "    sub = linedf[(linedf[\"scentroid\"]==row[\"scentroid\"]) & (linedf[\"acentroid\"]==row[\"acentroid\"])]\n",
    "    lines.append(sub)\n",
    "\n",
    "olines = pd.concat(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequences\n",
    "seq_dict = {record.description.split(\" \")[0]: str(record.seq) for record in SeqIO.parse(open(rootdir + \"trees/all_rps3.faa\"), \"fasta\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgolines = pd.concat([sglines, olines])\n",
    "sgolines = sgolines.merge(lineagg[[\"scentroid\", \"acentroid\", \"env_broad\", \"stax\", \"atax\", \"type\"]], how=\"left\", on=[\"scentroid\", \"acentroid\"])\n",
    "sgolines[\"sseq\"] = sgolines[\"scluster\"].map(seq_dict)\n",
    "sgolines[\"aseq\"] = sgolines[\"acluster\"].map(seq_dict)\n",
    "sgolines = sgolines[[\"sample\", \"env_broad\", \"scluster\", \"stax\", \"sseq\", \"acluster\", \"atax\", \"aseq\", \"type\"]]\n",
    "sgolines.columns = [\"run_id\", \"habitat_broad\", \"saccharibacateria_graftm_s3\", \"saccharibacteria_graftm_taxonomy\", \"saccharibacteria_graftm_seq\", \"actinobacteria_graftm_s3\", \"actinobacteria_graftm_taxonomy\", \"actinobacteria_graftm_seq\", \"co-occurrence type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgolines.sort_values([\"habitat_broad\", \"co-occurrence type\", \"run_id\"], ascending=[True,True, True]).to_csv(rootdir + \"metadata/supp_table_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gracili/sr1 co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"marker_gene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples of interest\n",
    "runs = covsub[covsub[\"taxcat\"].isin([\"Absconditabacteria\", \"Gracilibacteria\"])][\"run_id\"].unique()\n",
    "info = runsub[runsub[\"run_id\"].isin(runs)].drop_duplicates([\"run_id\", \"read_path\", \"curated_assembly_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute coverage values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = []\n",
    "\n",
    "for key, row in info.iterrows():\n",
    "    \n",
    "    temp = []\n",
    "    table = cresults[row[\"run_id\"]]\n",
    "    \n",
    "    # pull scafs\n",
    "    filename = rootdir + \"marker_gene/\" + row[\"run_id\"] + \".names.txt\"\n",
    "    with open(filename, \"w\") as names:\n",
    "        for centroid in table[\"centroid\"].unique():\n",
    "            names.write(\"_\".join(centroid.split(\"_\")[:-3]) + \"\\n\")\n",
    "    outfile = filename.replace(\"names.txt\", \"markerscafs.fna\")\n",
    "    call = \"pullseq -n \" + filename + \" -i \" + row[\"curated_assembly_path\"] + \\\n",
    "        \" > \" + outfile\n",
    "    sp.call(call, shell=True)\n",
    "    \n",
    "    temp.append(\"bowtie2-build \" + outfile + \" \" + outfile)\n",
    "    temp.append(\"bowtie2 -p 48 -x \" + outfile + \" -1 \" + \\\n",
    "        row[\"read_path\"].replace(\".fa.gz\", \".1.fastq.gz\") + \" -2 \" + \\\n",
    "        row[\"read_path\"].replace(\".fa.gz\", \".2.fastq.gz\") + \\\n",
    "        \" | shrinksam | samtools view -S -b > \" + \\\n",
    "        outfile.replace(\"fna\", \"bam\"))\n",
    "    temp.append(\"samtools sort --threads 48 \" + \\\n",
    "        outfile.replace(\"fna\", \"bam\") + \" > \" + outfile.replace(\"fna\", \"sorted.bam\"))\n",
    "    temp.append(\"samtools index -@ 48 \" + outfile.replace(\"fna\", \"sorted.bam\"))\n",
    "    mappings.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapperize(mappings, 10, rootdir + \"marker_gene/map\", \"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get non zero sorted bams\n",
    "sorted_bams = [item for item in glob.glob(rootdir + \"/marker_gene/*sorted.bam\") if os.stat(item).st_size != 0]\n",
    "\n",
    "for bam in sorted_bams:\n",
    "    \n",
    "    coverm = \"coverm contig --min-read-percent-identity .99 --output-format sparse -b \" + bam + \" -m\" + \\\n",
    "        \" count mean covered_fraction length rpkm > \" + rootdir + \"/marker_gene/\" + \\\n",
    "        os.path.basename(bam).replace(\"sorted.bam\", \"\") + \"coverage_table.tsv\"\n",
    "    sp.call(coverm, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_cov = pd.concat([pd.read_csv(table, sep=\"\\t\") for table in glob.glob(rootdir + \"marker_gene/*coverage_table.tsv\")])\n",
    "marker_cov = marker_cov[marker_cov[\"Covered Fraction\"]>0.10]\n",
    "msub = marker_cov[[\"Sample\", \"Contig\", \"Mean\"]]\n",
    "msub[\"run_id\"] = msub[\"Sample\"].apply(lambda x: x.split(\".\")[0])\n",
    "msub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge with clustering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cresults = pd.concat(cresults.values())\n",
    "all_cresults = all_cresults[[\"centroid\", \"taxstring\"]]\n",
    "all_cresults[\"Contig\"] = all_cresults[\"centroid\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-3]))\n",
    "all_cresults[\"lineage\"] = all_cresults[\"taxstring\"].apply(lambda x: x.split(\";\")[4] if len(x.split(\";\")) > 4 else x.split(\";\")[-1])\n",
    "all_cresults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "merge = msub.merge(all_cresults[[\"Contig\", \"lineage\"]], on=\"Contig\", how=\"left\")\n",
    "mgb = merge.groupby([\"run_id\", \"lineage\"], as_index=False).aggregate({\"Mean\":\"sum\"})\n",
    "\n",
    "# compute relative cov\n",
    "mgb = mgb.merge(mgb.groupby(\"run_id\", as_index=False).aggregate({\"Mean\":\"sum\"})\n",
    "    .rename(columns={\"Mean\":\"total\"}), how=\"left\", on=\"run_id\")\n",
    "mgb[\"rel_cov\"] = mgb.apply(lambda x: x[\"Mean\"]/float(x[\"total\"]), axis=1)\n",
    "mgb[\"scaled_rel_cov\"] = mgb[\"rel_cov\"].apply(lambda x: math.log10(x/float(min(mgb[\"rel_cov\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_data = covsub[[\"run_id\", \"env_broad\", \"env_narrow\"]].drop_duplicates()\n",
    "tallyho = mgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublineages = pd.DataFrame(tallyho[\"lineage\"].value_counts()).query(\"lineage>7\").index.to_list()\n",
    "sublineages = [item for item in sublineages if item not in \n",
    "    [' o__BD1-5',' o__Saccharimonadales',' d__Bacteria', ' o__UBA9983', 'o__UBA1400', ' o__Moranbacterales', ' c__ABY1', ' o__UBA6257', ' o__UBA1369']]\n",
    "sublineages = [ ' o__BD1-5' , ' o__Absconditabacterales', ' o__Saccharimonadales'] + sublineages\n",
    "tallysub = tallyho[tallyho[\"lineage\"].isin(sublineages)]\n",
    "tallypiv = tallysub.pivot(\"run_id\", \"lineage\", \"scaled_rel_cov\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presdict = {}\n",
    "\n",
    "for key, row in covsub.iterrows():\n",
    "    \n",
    "    if row[\"run_id\"] not in presdict:\n",
    "        presdict[row[\"run_id\"]] = set([row[\"taxcat\"]])\n",
    "    else:\n",
    "        presdict[row[\"run_id\"]].add(row[\"taxcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\" o__Saccharimonadales\", \" o__BD1-5\", \" o__Absconditabacterales\"]:\n",
    "    \n",
    "    if \"Sacchari\" in column:\n",
    "        name = \"Saccharibacteria\"\n",
    "    elif \"BD1-5\" in column:\n",
    "        name = \"Gracilibacteria\"\n",
    "    elif \"Abscondita\" in column:\n",
    "        name = \"Absconditabacteria\"\n",
    "    \n",
    "    temp = []\n",
    "    for key, row in tallypiv.iterrows():\n",
    "        if name in presdict[key]:\n",
    "            temp.append(max(tallyho[\"scaled_rel_cov\"]))\n",
    "        else: temp.append(0)\n",
    "    \n",
    "    tallypiv[name] = temp\n",
    "    tallypiv = tallypiv.drop(column, axis=1)\n",
    "    sublineages.remove(column)\n",
    "    sublineages.insert(0, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tallymerge = tallypiv.reset_index().merge(env_data, how=\"left\", on=\"run_id\")\n",
    "tallymerge[\"dummy\"] = 0\n",
    "sublineages.insert(3, \"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tallymerge.query(\"env_narrow=='wastewater'\").query(\"Absconditabacteria!=0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(tallymerge[\"env_broad\"].unique())\n",
    "count=1\n",
    "\n",
    "for env in tallymerge[\"env_broad\"].unique():\n",
    "    \n",
    "    data = tallymerge[tallymerge[\"env_broad\"]==env].sort_values([\"Gracilibacteria\", \"Absconditabacteria\"], ascending=[True, False])\n",
    "    data.index = data[\"env_narrow\"]\n",
    "    data = data.drop([\"run_id\", \"env_broad\", \"env_narrow\"], axis=1)\n",
    "    fraction = len(data)/float(len(tallymerge))\n",
    "    plt.figure(figsize=(9,12))\n",
    "    g= sns.heatmap(data[sublineages], linewidths=1, vmax=max(tallysub.scaled_rel_cov), \n",
    "        cmap=sns.light_palette(env2color[env], as_cmap=True), cbar=False, square=True)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    \n",
    "    if count != total:\n",
    "        plt.tick_params(bottom=False)\n",
    "        g.set(xticks=[])\n",
    "    else: plt.xticks(rotation=60, horizontalalignment=\"right\")\n",
    "        \n",
    "    plt.savefig(rootdir + \"figures/\" + env + \"_heatmap.svg\", format=\"svg\")\n",
    "    plt.show()\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reanalyze PH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"eel_river\")\n",
    "cmdir(rootdir + \"eel_river/graftm/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graftm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"eel_river/graftm_wrapper.sh\", \"w\") as outfile:\n",
    "\n",
    "    for assembly in glob.glob(datadir + \"PH*/assembly.d/*idba*/*_min1000.fa\"):\n",
    "\n",
    "        runid = os.path.basename(assembly).split(\"_scaffold\")[0]\n",
    "        call = \"graftM graft --threads 20 --forward \" + \\\n",
    "            assembly + \" --input_sequence_type nucleotide --graftm_package \" + gpkg + \\\n",
    "            \" --filter_minimum 0 --output_directory \" + rootdir + \"/eel_river/graftm/\" + runid\n",
    "        outfile.write(call + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in results\n",
    "results = {}\n",
    "\n",
    "for graftdir in glob.glob(rootdir + \"eel_river/graftm/*/*/\"):\n",
    "    \n",
    "    name = graftdir.split(\"/\")[-2].split(\"_scaffold\")[0]\n",
    "    # find files\n",
    "    taxinfo = glob.glob(graftdir + \"/*read_tax.tsv\")[0]\n",
    "    results[name] = {\"tax\": pd.read_csv(taxinfo, sep=\"\\t\", header=None)}\n",
    "    # only proceed if results\n",
    "    if os.stat(taxinfo).st_size != 0:\n",
    "        full_fasta = glob.glob(graftdir + \"*orf.fa\")[0]\n",
    "        results[name][\"fasta\"] = full_fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"eel_river/all_markers.faa\", \"w\") as outfile:\n",
    "    for sample in results.keys():\n",
    "        for record in SeqIO.parse(open(results[sample][\"fasta\"]), \"fasta\"):\n",
    "            outfile.write(\">\" + record.description.split(\" \")[0] + \"\\n\" + str(record.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call = \"usearch -cluster_fast \" + rootdir + \"/eel_river/all_markers.faa -sort length -id 0.97 \" + \\\n",
    "    \"-centroids \" + rootdir + \"/eel_river/all_markers.centroids\" + \\\n",
    "    \" -uc \" + rootdir + \"/eel_river/all_markers.clusters\"\n",
    "sp.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eelclust = parse_usearch_clustering(rootdir + \"eel_river/all_markers.clusters\")\n",
    "repscafs = [\"_\".join(gene.split(\"_\")[:-3]) for gene in eelclust[\"centroid\"].unique()]\n",
    "len(repscafs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(rootdir + \"eel_river/eel_markerscafs.fna\", \"w\") as outfile:\n",
    "    for assembly in glob.glob(datadir + \"PH*/assembly.d/*idba*/*_min1000.fa\"):\n",
    "        for record in SeqIO.parse(open(assembly), \"fasta\"):\n",
    "            if record.description.split(\" \")[0] in repscafs:\n",
    "                outfile.write(\">\" + record.description.split(\" \")[0] + \"\\n\" + str(record.seq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdir(rootdir + \"eel_river/mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapwrap = open(rootdir + \"eel_river/mapwrap.sh\", \"w\")\n",
    "mscafs = rootdir + \"eel_river/eel_markerscafs.fna\"\n",
    "mapwrap.write(\"bowtie2-build \" + mscafs + \" \" + mscafs + \"\\n\")\n",
    "\n",
    "for read in glob.glob(datadir + \"PH*/raw.d/*trim_clean.PE.1.fastq.gz\"):\n",
    "    \n",
    "    name = os.path.basename(read).split(\"_trim\")[0]\n",
    "    out = rootdir + \"eel_river/mapping/\" + name + \".bam\"\n",
    "    mapwrap.write(\"bowtie2 -p 20 -x \" + mscafs + \" -1 \" + \\\n",
    "        read + \" -2 \" + read.replace(\".1.\", \".2.\") + \\\n",
    "        \" | shrinksam | samtools view -S -b > \" + \\\n",
    "        out + \"\\n\")\n",
    "    mapwrap.write(\"amtools sort --threads 20 \" + \\\n",
    "        out + \" > \" + out.replace(\"bam\", \"sorted.bam\") + \"\\n\")\n",
    "    mapwrap.write(\"samtools index -@ 18 \" + out.replace(\"bam\", \"sorted.bam\") + \"\\n\")\n",
    "    \n",
    "mapwrap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract coverage information\n",
    "coverm = \"coverm contig --min-read-percent-identity .97 --output-format sparse -b \" + rootdir + \\\n",
    "    \"eel_river/mapping/*sorted.bam -m count mean covered_fraction length rpkm > \" + rootdir + \"eel_river/coverage_table.csv\"\n",
    "print(coverm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_cov = pd.read_csv(rootdir + \"eel_river/coverage_table.csv\", sep=\"\\t\")\n",
    "marker_cov = marker_cov[marker_cov[\"Covered Fraction\"]>0.10]\n",
    "msub = marker_cov[[\"Sample\", \"Contig\", \"Mean\"]]\n",
    "msub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eel = pd.concat(item[\"tax\"] for key, item in results.items())\n",
    "all_eel.columns = [\"gene\", \"tax\"]\n",
    "all_eel[\"Contig\"] = all_eel[\"gene\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-3]))\n",
    "mmerge = msub.merge(all_eel[[\"Contig\",\"tax\"]], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corrs = defaultdict(list)\n",
    "\n",
    "piv = mmerge.pivot(\"Sample\", \"Contig\", \"Mean\").fillna(0)\n",
    "\n",
    "for col in piv.columns:\n",
    "    if col != \"PH2015_14_scaffold_705\":\n",
    "        r, pval = pearsonr(piv[\"PH2015_14_scaffold_705\"], piv[col])\n",
    "        corrs[\"row\"].append(\"PH2015_14_scaffold_705\")\n",
    "        corrs[\"col\"].append(col)\n",
    "        corrs[\"r\"].append(r)\n",
    "        corrs[\"pval\"].append(pval)\n",
    "        \n",
    "corrdf = pd.DataFrame(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdfs = {}\n",
    "\n",
    "for i,col in enumerate(corrdf.sort_values(\"r\", ascending=False)[\"col\"][0:9]):\n",
    "    subtable = piv[[\"PH2015_14_scaffold_705\", col]].reset_index()\n",
    "    subtable[\"taxon\"] = col\n",
    "    subtable.columns = [\"sample\", \"sr1_mean_cov\", \"taxon_mean_cov\", \"taxon\"]\n",
    "    subdfs[i] = subtable\n",
    "\n",
    "newdata = pd.concat(subdfs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = corrdf.sort_values(\"r\", ascending=False).merge(mmerge[[\"Contig\", \"tax\"]], left_on=\"col\", right_on=\"Contig\", how=\"left\").drop_duplicates().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp = top10[[\"col\", \"tax\", \"r\", \"pval\"]]\n",
    "supp.columns = [\"representative_scaffold\", \"graftm_taxonomy\", \"r\", \"pval\"]\n",
    "supp.to_csv(rootdir + \"metadata/eel_supp_table.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
